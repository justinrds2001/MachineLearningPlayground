{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression in practice - evaluation metrics - threshold tuning\n",
    "\n",
    "This hands-on workshop shows:\n",
    "* how to do logistic regression in practice using the sci-kit learn library\n",
    "* several evaluation metrics, like auroc and f-score\n",
    "* how to optimize the threshold\n",
    "\n",
    "Explanation about precision, recall and F-score by Andrew Ng:\n",
    "* [Error Metrics For Skewed Classes](https://www.youtube.com/watch?v=wGw6R8AbcuI&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=67) (12 min).\n",
    "* [Trading Off Precision And Recall](https://www.youtube.com/watch?v=W5meQnGACGo&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=68) (14 min).\n",
    "\n",
    "This workshop is heavily based on [A Gentle Introduction to Threshold-Moving for Imbalanced Classification](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/), [Understanding AUC - ROC Curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) and [Classification: ROC Curve and AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Probabilities to Class Labels\n",
    "\n",
    "Many machine learning algorithms are capable of predicting a probability of class membership. This is useful generally as it provides a measure of the certainty or uncertainty of a prediction.\n",
    "\n",
    "Some classification tasks require a crisp class label prediction. This means that even though a probability of class membership is predicted, it must be converted into a crisp class label.\n",
    "\n",
    "The decision for converting a predicted probability into a class label is governed by a hyperparameter referred to as the *decision threshold*. The default value for the threshold is 0.5. For example, on a binary classification problem with class labels 0 and 1, then values less than the threshold of 0.5 are assigned to class 0 and values greater than or equal to 0.5 are assigned to class 1.\n",
    "* Prediction < 0.5 => Class 0\n",
    "* Prediction >= 0.5 => Class 1\n",
    "\n",
    "The problem is that the default threshold may not represent an optimal interpretation of the predicted probabilities. This might be the case if the class distribution is severely skewed, for example in the case of cancer screening, 95.5% will turn out healthy, wherease only 0.5% of the people will be diagnosed cancer. Another reason why the default threshold of 0.5 might not be optimal is if the cost of one type of misclassification is more important than another type of misclassification. As such, there is often the need to change the default decision threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Threshold for ROC Curve\n",
    "\n",
    "Let's assume that the positive class (y=1) denotes that the person is diagnosed with cancer. Let's assume that the data set is heavily skewed anf that the positive class is the minority class.\n",
    "\n",
    "**ROC curve**. The false-positive rate is plotted on the x-axis and the true-positive rate is plotted on the y-axis and the plot is referred to as the Receiver Operating Characteristic curve, or ROC curve. A diagonal line on the plot from the bottom-left to top-right indicates the \"curve\" for a *no-skill classifier* (predicts the majority class in all cases), and a point in the top left of the plot indicates a model with perfect skill.\n",
    "\n",
    "The ROC curve is a useful diagnostic tool for understanding the trade-off in the true-positive rate and false-positive rate for different thresholds. The area under the ROC Curve, so-called ROC AUC, provides a single number to summarize the performance of the model in terms of its ROC Curve with a value between 0.5 (no-skill) and 1.0 (perfect skill). If crisp class labels are required from the model, then an optimal threshold is required. This would be a threshold on the curve that is closest to the top-left of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve for logistic regression model\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, \n",
    "    random_state=4)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "\n",
    "# predict probabilities on the test set\n",
    "yhat = model.predict_proba(testX)\n",
    "# yhat contains two columns, 1st colum: probability for negative outcome, 2nd column: probability for positive outcome\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 1]\n",
    "\n",
    "# calculate roc curves\n",
    "fpr, tpr, thresholds = roc_curve(testy, yhat)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits a logistic regression model on the training dataset then evaluates it using a range of thresholds on the test set, creating the ROC Curve. We can see that there are a number of points or thresholds close to the top-left of the plot. Which is the threshold that is optimal?\n",
    "\n",
    "There are many ways we could locate the threshold with the optimal balance between false positive and true positive rates. Firstly, the true positive rate is called the Sensitivity. The inverse of the false-positive rate is called the Specificity.\n",
    "* Sensitivity = TruePositive / (TruePositive + FalseNegative)\n",
    "* Specificity = TrueNegative / (FalsePositive + TrueNegative)\n",
    "\n",
    "Where:\n",
    "* Sensitivity = True Positive Rate\n",
    "* Specificity = 1 â€“ False Positive Rate\n",
    "\n",
    "The Geometric Mean or G-Mean is a metric for imbalanced classification that, if optimized, will seek a balance between the sensitivity and the specificity.\n",
    "\n",
    "G-Mean = sqrt(Sensitivity * Specificity)\n",
    "\n",
    "Given that we have already calculated the Sensitivity (TPR) and the complement to the Specificity when we calculated the ROC Curve, we can calculate the G-Mean for each threshold directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the point for the optimal threshold is a large black dot and it appears to be closest to the top-left of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Threshold for Precision-Recall Curve\n",
    "\n",
    "Unlike the ROC Curve, a precision-recall curve focuses on the performance of a classifier on the positive (minority class) only.\n",
    "\n",
    "* Precision = TruePositive / (TruePositive + FalsePositive)\n",
    "* Recall = TruePositive / (TruePositive + FalseNegative)\n",
    "\n",
    "Precision describes how good a model is at predicting the positive class. Recall is the same as sensitivity.\n",
    "\n",
    "A precision-recall curve is calculated by creating crisp class labels for probability predictions across a set of thresholds and calculating the precision and recall for each threshold. A line plot is created for the thresholds in ascending order with recall on the x-axis and precision on the y-axis.\n",
    "\n",
    "A no-skill model is represented by a horizontal line with a precision that is the ratio of positive examples in the dataset (e.g. TP / (TP + TN)), or 0.01 on our synthetic dataset. A perfect skill classifier has full precision and recall with a dot in the top-right corner.\n",
    "\n",
    "We can use the same model and dataset from the previous section and evaluate the probability predictions for a logistic regression model using a precision-recall curve. The precision_recall_curve() function can be used to calculate the curve, returning the precision and recall scores for each threshold as well as the thresholds used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pr-curve\n",
    "precision, recall, thresholds = precision_recall_curve(testy, yhat)\n",
    "\n",
    "# plot the pr-curve for the model\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Logistic')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we required crisp class labels from this model, which threshold would achieve the best result?\n",
    "\n",
    "If we are interested in a threshold that results in the best balance of precision and recall, then this is the same as optimizing the F-score that summarizes the harmonic mean of both measures.\n",
    "\n",
    "F-score = (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "The following code converts the precision and recall to the F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to f score\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "\n",
    "# plot the pr-curve for the model\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Logistic')\n",
    "plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This threshold associated with the highest F-score, could then be used when making probability predictions in the future that must be converted from probabilities to crisp class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Threshold Tuning\n",
    "\n",
    "Sometimes, we simply have a model and we wish to know the best threshold directly. In this case, we can define a set of thresholds and then evaluate predicted probabilities under each in order to find and select the optimal threshold.\n",
    "\n",
    "First, we fit a logistic regression model on our synthetic classification problem, then predict class labels and evaluate them using the F-Measure, which is the harmonic mean of precision and recall. This will use the default threshold of 0.5 when interpreting the probabilities predicted by the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for imbalanced classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict labels\n",
    "yhat = model.predict(testX)\n",
    "# evaluate the model\n",
    "score = f1_score(testy, yhat)\n",
    "print('F-Score: %.5f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we can see that the model achieved an F-Measure of about 0.70 on the test dataset.\n",
    "\n",
    "Next, we can then define a set of thresholds to evaluate the probabilities. In this case, we will test all thresholds between 0.0 and 1.0 with a step size of 0.001, that is, we will test 0.0, 0.001, 0.002, 0.003, and so on to 0.999.\n",
    "\n",
    "Next, we need a way of using a single threshold to interpret the predicted probabilities.\n",
    "\n",
    "This can be achieved by mapping all values equal to or greater than the threshold to 1 and all values less than the threshold to 0. We will define a to_labels() function to do this that will take the probabilities and threshold as an argument and return an array of integers in {0, 1}.\n",
    "\n",
    "We can then call this function for each threshold and evaluate the resulting labels using the f1_score().\n",
    "\n",
    "We can do this in a single line, as follows:\n",
    "\n",
    "We now have an array of scores that evaluate each threshold in our array of thresholds.\n",
    "\n",
    "All we need to do now is locate the array index that has the largest score (best F-Measure) and we will have the optimal threshold and its evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search thresholds for imbalanced classification\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    "\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "yhat = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(testy, to_labels(probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example reports the optimal threshold as 0.251 (compared to the default of 0.5) that achieves an F-Measure of about 0.75 (compared to 0.70)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "ROC or F-score?\n",
    "* https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc\n",
    "* https://stats.stackexchange.com/questions/210700/how-to-choose-between-roc-auc-and-f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
