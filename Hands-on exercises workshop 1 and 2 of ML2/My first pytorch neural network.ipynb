{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6332ba",
   "metadata": {},
   "source": [
    "# Pytorch 3x4x2 model \n",
    "\n",
    "Purpose of this notebook is to show Pytorch notation for building neural networks, as opposed to doing it using Keras/Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b36607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "n_input = 3 # number of features\n",
    "n_hidden = 4 # number of hidden layers\n",
    "n_out = 2 # number of outputs\n",
    "batch_size = 64 # batch size for mini-batch gradient descent\n",
    "learning_rate = 0.01\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('iris.csv')\n",
    "X = df.iloc[:,:3].to_numpy() # only use 1st 3 features (not logical, but to mimic the 3x4x2 model in the slides)\n",
    "y = df.iloc[:,-3:-1].to_numpy() # the 3rd and 2nd columns from the right are the label (not logical, but to mimic the 3x4x2 model in the slides)\n",
    "\n",
    "# transform to pytorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# create model\n",
    "model = nn.Sequential(nn.Linear(n_input, n_hidden),\n",
    "                      nn.Tanh(), # activation function\n",
    "                      nn.Linear(n_hidden, n_out),\n",
    "                      nn.Sigmoid()) # activation function\n",
    "loss_function = nn.BCELoss() # binary cross-entropy\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) # stochastic gradient descent\n",
    "\n",
    "# visualize model architecture\n",
    "print(model)\n",
    "\n",
    "# train model\n",
    "for epoch in range(5000): # go 5000 times through the complete dataset\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        y_pred = model(X_batch) # perform forward pass (== do a prediction)\n",
    "        loss = loss_function(y_pred, y_batch) # calculate loss\n",
    "        optimizer.zero_grad() # reset gradient to zero, otherwise the gradient of previous batches is added\n",
    "        loss.backward() # compute the gradients of all weights and biases\n",
    "        optimizer.step() # update all weights and biases\n",
    "    print(f'\\rfinished training of epoch {epoch}', end=\"\")\n",
    "\n",
    "# compute accuracy on *train* set (no_grad is optional)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "accuracy1 = (y_pred[:,0].round() == y[:,0]).float().mean()\n",
    "accuracy2 = (y_pred[:,1].round() == y[:,1]).float().mean()\n",
    "print(f\"\\nAccuracy on each column: [{accuracy1}, {accuracy2}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fe181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310] *",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
