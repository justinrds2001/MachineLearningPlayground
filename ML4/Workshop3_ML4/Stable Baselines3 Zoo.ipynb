{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d405af2f",
   "metadata": {},
   "source": [
    "# Getting our snake into the zoo!\n",
    "\n",
    "<img src=\"stablebaselineszoo.jpg\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Stable Baselines3 Zoo is a training framework for RL. It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos. Tuning hyperparameters is done via [Optuna](https://optuna.org/), a hyperparameter optimization framework.\n",
    "\n",
    "Zoo [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html) and [repo README](https://github.com/DLR-RM/rl-baselines3-zoo).\n",
    "\n",
    "## Using Zoo\n",
    "\n",
    "### Installing zoo\n",
    "\n",
    "* `git clone https://github.com/DLR-RM/rl-baselines3-zoo`\n",
    "* `cd rl-baselines3-zoo`\n",
    "* `pip install -e .`\n",
    "\n",
    "### Prepare zoo for usage with our snake\n",
    "\n",
    "Start situation is that the `gymsnake` module has been installed using `pip install -e .`, which has registered it with OpenAI Gym under name `snake-v1`.\n",
    "\n",
    "* In file `rl_zoo3/import_envs.py` add the following lines to import a custom environment, in this case the snake environment:\n",
    "```\n",
    "try:\n",
    "    import gymsnake\n",
    "except ImportError:\n",
    "    gymsnake = None\n",
    "```\n",
    "* In file `hyperparameters/dqn.yml` add at the end (after 1st line indented with two spaces):\n",
    "```\n",
    "# not yet tuned!\n",
    "snake-v1:\n",
    "    normalize: \"{'norm_obs': True, 'norm_reward': True}\"\n",
    "    n_timesteps: !!float 1e6\n",
    "    policy: 'MlpPolicy'\n",
    "    learning_rate: 0.0001\n",
    "    batch_size: 32\n",
    "    buffer_size: 1000000\n",
    "    learning_starts: 50000\n",
    "    gamma: 0.99\n",
    "    target_update_interval: 10000\n",
    "    train_freq: 4\n",
    "    gradient_steps: 1\n",
    "    exploration_fraction: 0.1\n",
    "    exploration_final_eps: 0.05\n",
    "    policy_kwargs: \"dict(net_arch=[256, 256])\"\n",
    "```\n",
    "* In file `hyperparameters/ppo.yml` add at the end (after 1st line indented with two spaces):\n",
    "```\n",
    "# not yet tuned!\n",
    "snake-v1:\n",
    "    normalize: \"{'norm_obs': True, 'norm_reward': True}\"\n",
    "    n_envs: 1\n",
    "    policy: 'MlpPolicy'\n",
    "    n_timesteps: !!float 1e6\n",
    "    batch_size: 64\n",
    "    n_steps: 2048\n",
    "    gamma: 0.99\n",
    "    learning_rate: 0.0003\n",
    "    ent_coef: 0.0\n",
    "    clip_range: 0.2\n",
    "    n_epochs: 10\n",
    "    gae_lambda: 0.95\n",
    "    max_grad_norm: 0.5\n",
    "    vf_coef: 0.5\n",
    "```\n",
    "\n",
    "\n",
    "### Some examples of using zoo\n",
    "\n",
    "Parameters on the command line override parameters in `hyperparameters/dqn.yml`.\n",
    "\n",
    "**Training:**\n",
    "  * `python train.py --verbose 1 --tensorboard-log tensorboard_log --algo ppo --env snake-v1 --env-kwargs \"grid_size:[6, 6]\" --n-timesteps 1000000` train snake with zoo using the hyperparameters as specified in `hyperparameters/dqn.yml`, evaluate the model every 10000 steps (default eval_freq is 10000) and save the model at the end of the session in `snake-v1.zip`. Evaluating means: the performance of the last 5 episodes is evaluated and the model is saved in `best_model.zip` if it is better than the best model of all previous evaluations. \n",
    "  * `python train.py --verbose 1 --tensorboard-log tensorboard_log --algo ppo --env snake-v1 --env-kwargs \"grid_size:[6, 6]\" --n-timesteps 1000000 --device cpu` as previous, but on cpu.\n",
    "  * `python train.py --verbose 1 --tensorboard-log tensorboard_log --algo ppo --env snake-v1 --env-kwargs \"grid_size:[6, 6]\" --eval-freq 1000 --save-freq 20000 --n-timesteps 1000000` as previous, but with intermediary evaluation every 1000 steps and intermediary saving of the model every 20000 steps, ending up with 50 saved models as the number of steps is 1000000. \n",
    "  * `python train.py --verbose 1 --tensorboard-log tensorboard_log --algo ppo --env snake-v1 --env-kwargs \"grid_size:[6, 6]\" -i logs/ppo/snake-v1_1/best_model.zip --n-timesteps 1000000` as previous but continue training a preloaded model, in this case the best model of the mentioned experiment.\n",
    "\n",
    "**Enjoying a trained model:**\n",
    "  * `python enjoy.py --verbose 1 --algo ppo --env snake-v1 --n-timesteps 50 --folder logs/` enjoy the **last** saved model from the **last** experiment.\n",
    "  * `python enjoy.py --verbose 1 --algo ppo --env snake-v1 --folder logs/ --load-best` enjoy the **best** saved model from the **last** experiment.\n",
    "  * `python enjoy.py --verbose 1 --algo ppo --env snake-v1 --folder logs/ --load-best --exp-id 5` enjoy the **best** saved model from the experiment with id 5.\n",
    "\n",
    "**Hyperparameter optimization (typically prior to training):**\n",
    "  * `python train.py --verbose 1 --algo ppo --env snake-v1 --env-kwargs \"grid_size:[6, 6]\" -n 1000000 -optimize --n-trials 1000 --n-jobs 2 --sampler random --pruner median` use Optuna for hyperparameter optimization (1000 trials of 1000000 steps each, 2 parallel jobs).\n",
    "\n",
    "The latter command first loads the hyperparameters as specified in `hyperparameters/ppo.yml` and then uses the the values in `rl_zoo3/hyperparams_opt.py` as the hyperparameter search space. It uses a random sampler and median pruner ([Optuna tutorial](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html#)). The number of trials is the number of hyperparameter combinations it will run; it's the time budget that you want to spend on hyperparameter optimization. It takes several days to run (°◇°).\n",
    "\n",
    "All results of a training run or optimization run (used hyperparameter settings, best hyperparameter settings, tensorboard logs, trained models) are stored in the folder `logs/ppo/snake-v1_1`, so a separate folder for each run.\n",
    "\n",
    "## Vectorized environments for the AAI-lab\n",
    "\n",
    "The AAI-lab has 64 CPUs, so 128 virtual CPUs. Stable Baselines3 has so-called [vectorized environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html) that allow using CPU's in parallel. Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on `1` environment per step, it allows us to train it on  `n` environments per step.\n",
    "\n",
    "* in `hyperparameters/ppo.yml` set `n_envs: 5`. This will created `5` parallel snake environments.\n",
    "* `python train.py --algo ppo --env snake-v1 --vec-env subproc --device cpu --num-threads 10` to use vectorized environments from zoo.\n",
    "\n",
    "Some remarks:\n",
    "* Running on CPU seems to be faster than running on GPU.\n",
    "* At least 128 threads would be a logical value, but this does not give good results.\n",
    "* Vectorized environments come with a lot of multi-process and multi-thread overhead. `top` might show very busy CPUs, but the `fps` (frames per second) shown by Stable Baselines 3 might still be quite low. You therefore need to play with the hyperparams `n_env` and `num_threads`. Simple environments with little calculations might not benefit at all from vectorized environments.\n",
    "* I've not yet grasped all ins & outs of vectorized environments. For examples observation, done, reward, action now become vectors. Is this done automatically by the VecEnvWrapper of does this need to be done manually in the gymsnake environment? There's still some experimentation to be done.\n",
    "\n",
    "Of course, vectorized environments can also be used directly from Stable Baselines, so outside from Zoo.\n",
    "\n",
    "## Getting learning behavior!\n",
    "\n",
    "We've got a basic understanding of RL. We've setup Zoo, allowing us to efficiently run experiments. We're fully equipped to learn our snake to behave! Let the fun begin! This section discusses pitfalls, strategies, considerations and not-tested hypotheses. Note the strategies are not exclusive to each other; they might be combined.\n",
    "\n",
    "**Strategy: play with number of steps**\n",
    "The first basic strategy that you'll probably try:\n",
    "1. Make an intelligent guess for the values of the hyperparameters. Very often this will be the default values. Unfortunately, default values of Stable Baselines3 have not been chosen to give decent initial performance. Maybe this is because different environments ask for very different values of hyperparameters, meaning that smart default values is simply not possible. I'm not sure.\n",
    "2. Choose a number of steps, train the agent and monitor how `ep_rew_mean` is evolving in Tensorboard. As long as `ep_rew_mean` increases, longer training makes sense. Here it comes in handy that interrupting a training session with Ctrl-c will save the model before stopping the session.\n",
    "3. Enjoy the result :).\n",
    "\n",
    "**Strategy: tune hyperparameters before training**\n",
    "1. Prepare `hyperparameters/ppo.yml` to your best knowledge.\n",
    "2. Do an optimize run with limited number of timesteps (e.g. 50000) for let's say 100 trials. This takes 4-5 hours for gymsnake.  Monitor that `max steps reached` is not occurring at a regular basis, as this might mean that you're cutting off wanted snake behavior. \n",
    "3. Do a long training with 5000000 timesteps with tensorboard logging enabled, using the hyperparameter values of the best trial of the optimization step. Monitor how `ep_rew_mean` is evolving. Increase the number of steps if `ep_rew_mean` is still increasing (in Tensorboard, move the smoothin slider to the right to allow assessing this).\n",
    "4. Enjoy the result :).\n",
    "\n",
    "A possible (i.e. not tested) pitfall of this strategy is that the optimal hyperparameter settings are determined using rather short training sessions. For example, in step 2. Optuna might suggest a small neural network as, during the (short) trial, it showed fast learning. However, during the long training session of step 3. the snakes will be longer in general and a bigger neural network might be needed to deal with such long snakes.\n",
    "\n",
    "**Strategy: staged learning**\n",
    "Instead of starting each training session with a new, empty model, start of with an already trained model. This provides many possibilities:\n",
    "* start of with a model that has expert knowledge in it to get decent behavior. Continue with a training session without expert knowledge to allow finding the optimal policy.\n",
    "* start with a training session with subgoals to at least find some rewards. Continue with a training session without suboals to allow finding the optimal policy. Typically useful for problems where there's only a reward at the very end of an episode like chess. \n",
    "* start with a training session with a small initial snake length. Continue with a training session with a longer intial snake length (requires source code changes in gymsnake). This allows later training runs to focus on long snakes. Without this, every episode starts with a small snake, so a lot of training time is lost training small snakes, whereas the model is not learning a lot any more for small snakes. It is important that the long snake is initialized at a random position, otherwise a large part of the state space is not examined. As well, it is important to increase the initial snake length not too fast, again to avoid missing a large part of the state space.\n",
    "\n",
    "Note that the hyperparameters that vary during a training session (e.g. exploration rate) are reinitialized with every training session. Splitting the training sessions in multiple sessions means that at the start of every session there's a lot of exploration. This might ruin the neural network. Or ... it might be beneficial as the snake examines new, possible better parts of the state space.\n",
    "\n",
    "Note that some hyperparameters (e.g. learning rate) and also aspects of the environment like reward signal can be changed between staged training session, but some hyperparameters (e.g. network architecture) cannot. \n",
    "\n",
    "**Literature intermezzo**\n",
    "An environment that returns few rewards (like chess where there's only a reward at the very end of an episode) is called an 'RL-problem with sparse rewards' in literature. The sparse rewards are the reason that getting good results is so hard in RL compared to supervised learning, because the feedback whether an action is good or bad only comes in the far future. This is called 'sampling inefficiency'. In contrast, in supervised learning the system gets feedback every sample, whether the system predicted the label correctly. Adding a subgoal like 'occupy the center of the board' is called in literature 'adding a dense reward', resulting in s system tha resembles more supervised learning. An example of a dense reward often seen in snake is the distance + direction to the food. In this way the snake get a reward every step. Designing reward signals is called 'reward shaping' in literature. A second problem of the sparse, future rewards is, what if the agent performed a chain of good actions, but made an error with the last action, therefore missing the reward? All taken actions, also the whole chain of good ones will be discredited. This is called 'the credit assignment problem'in literature.\n",
    "\n",
    "**Strategy: tweak the reward signal (reward shaping)**\n",
    "The reward signal is central in reinforcement learning and therefore the design of the reward signal deserves a lot of attention. Try philosophizing about the resulting snake behavior of a certain reward signal. Some examples of unexpected results (called 'the alignment problem' in literature):\n",
    "* dying is better than walking (reward: -1 per step and a -1 for dying)\n",
    "* turn-based duosnake: one snake kills itself immediately, after which the other snake quietly collects food (reward: the snake that has the turn gets +1 for finding food; -1 for dying)\n",
    "\n",
    "Actually the holy grail of RL is ***not*** needing reward shaping (== adding subgoals), because (1) it is a custom process that has to be done for every new environment again, (2) because of the alignment problem, and (3) because it constrains the agent policy to the behavior of humans, which is not true optimal behavior.\n",
    "\n",
    "**Strategy: start off simple, BUT ...**\n",
    "Already mentioned several times: KISS. Start off with a simple version of the problem (e.g. small grid for snake). Only when you observe decent learning behavior, increase complexity. There's one important pitfall here! If you start off with a problem that's **too simple**, finding a reward is not a matter of skill but of luck!! This means that the agent receives random rewards and will not learn. We saw this in the workshop about neuroevolution. \n",
    "\n",
    "**Problem: decreasing rewards**\n",
    "When during a training run, the reward first increases, but then decreases. Possible counter strategies ([source with explanation of causes](https://stackoverflow.com/questions/51960225/dqn-average-reward-decrease-after-training-for-a-period-of-time) and a [second source with explanation of causes](https://www.reddit.com/r/reinforcementlearning/comments/9zwr0r/why_do_rewards_start_to_drop_after_a_certain/)):\n",
    "* (not tested) rather than a constant learning rate, decrease the learning rate during the training session\n",
    "* (not tested) rather than a constant batch size, increase the batch size during the training session\n",
    "* (not tested) rather than a prioritized experience replay, decrease prioritized experience replay during the training session\n",
    "\n",
    "**Pitfall: max steps per episode**\n",
    "It is vital that an episode stops after a number of steps, otherwise a lot of training time can be lost in a forever cycling snake; only an exploration step can get the snake out of this endless loop. This functionality need not be implemented by the environment itself, as it is provided by a Open AI Gym wrapper (`miniconda3\\envs\\sb3\\Lib\\site-packages\\gym\\wrappers\\time_limit.py`). The value of the maximum number of steps per episode can be set in `gymsnake/__init__.py` by means of the line `max_episode_steps=1500,`. A too low value might cut off wanted snake behavior. A too high value means that a lot of training time is lost in endless loops. Monitor whether `max steps reached` is not happening too often.\n",
    "\n",
    "**Pitfall: normalized observations and normalized rewards**\n",
    "Many RL algorithms rely on a Gaussian underlying distribution. Therefore it is important to normalize observations and rewards. This can be hard-coded in the environment, but it is also possible to use the wrapper `VecNormalize`. In Zoo, this wrapper can be enabled by hyperparameter `normalize: \"{'norm_obs': True, 'norm_reward': True}\"`. [More info in section 1.3.3](https://buildmedia.readthedocs.org/media/pdf/stable-baselines/master/stable-baselines.pdf).\n",
    "\n",
    "**Pitfall: normalized actions**\n",
    "Many RL algorithms, typically continuous action spaces, rely on a normalized and symmetric action space. This must be hard-coded in the environment. [More info in section 1.3.3](https://buildmedia.readthedocs.org/media/pdf/stable-baselines/master/stable-baselines.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d00e13f",
   "metadata": {},
   "source": [
    "## Using Zoo in Google Colab\n",
    "\n",
    "Instead of installing it locally, it is easier to create a copy of this [Google Colab notebook](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb), but ... the notebook disconnects after 90 minutes of idle time and has a maximum running time of 12 hours. All data is then lost if you didn't download it before disconnect. So the ease of installation comes at a price. It's a good option for quickly assessing the usefulness of Zoo.\n",
    "\n",
    "* create a zip of gymsnake gymsnake.zip\n",
    "* upload the zip to Google Colab\n",
    "* within Google Colab:\n",
    "```\n",
    "%cd /content/rl-baselines3-zoo/\n",
    "!unzip /content/gymsnake.zip\n",
    "%cd /content/rl-baselines3-zoo/gymsnake/\n",
    "!pip install -e .\n",
    "```\n",
    "`pip install -e .` installs the gymsnake module and registers the snake environment with open AI Gym under name `snake-v1`.\n",
    "\n",
    "* In file `train.py` add the line `import gymsnake` below the line `import gym`. \n",
    "* within Google Colab:\n",
    "```\n",
    "%cd /content/rl-baselines3-zoo/\n",
    "!python train.py --algo dqn --env snake-v1 --n-timesteps 100000\n",
    "```\n",
    "* An alternative for uploading a zip to Google Colab is to mount your Google Drive. To do this, within Google Colab:\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "```\n",
    "\n",
    "### Preventing a Google Colab notebook from closing if untoched for a while\n",
    "\n",
    "**Not sure whether this workaround still works.**\n",
    "\n",
    "* in the Chrome window where Colab is running, right-click and choose 'inspect'\n",
    "* paste the javascript code below in the console: \n",
    "```\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\"); \n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "}\n",
    "var colab = setInterval(ConnectButton,60000);\n",
    "```\n",
    "\n",
    "* when your colab session is ready you want to stop the timer. To stop the timer paste the javascript code below in the console:\n",
    "```\n",
    "clearInterval(colab)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1116a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
