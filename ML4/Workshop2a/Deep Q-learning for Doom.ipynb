{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning (DQN) for Doom\n",
    "\n",
    "![](vizdoom.png)\n",
    "\n",
    "## Doom game rules, the BASIC scenario\n",
    "\n",
    "* The map is a rectangle with walls, ceiling and floor\n",
    "* A monster is spawned randomly somewhere along the opposite wall\n",
    "* The player can only go left/right or shoot\n",
    "* One hit is enough to kill the monster\n",
    "* Episode finishes when monster is killed or on timeout (300 tics).\n",
    "\n",
    "Rewards:\n",
    "* +100 for killing the monster\n",
    "* -1 for every time tick (every time tick there's an action left/right/shoot)\n",
    "* -5 missed shot\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "No specific reason for these specific versions, just that I used them and they worked. To prevent problems, you could you use these versions as well, although newer versions are available.\n",
    "\n",
    "* python 3.7\n",
    "* tensorflow 1.15\n",
    "* skimage latest version (conda install scikit-image)\n",
    "* vizdoom 1.1.7\n",
    "\n",
    "Installing vizdoom is easiest without using conda or pip:\n",
    "1. download version 1.1.7 from https://github.com/mwydmuch/ViZDoom/releases\n",
    "2. unpack the zip in `...\\Anaconda3\\envs\\<your-conda-env>\\Lib\\site-packages`\n",
    "3. you should now have a folder named `vizdoom` in the folder `site-packages`\n",
    "4. you will find a scenario folder in the vizdoom folder, copy basic.cfg and basic.wad which are inside the scenario folder and put them in the same folder as your own code. This step assumes you are trying to play the basic mission.\n",
    "5. if your python version doesn't match change version in file `...\\Anaconda3\\envs\\<your-conda-env>\\Lib\\site-packages\\vizdoom\\__init__.py`\n",
    "6. in basic.cfg (the version in the folder with your own code), change to \"screen_format = GRAY8\"\n",
    "7. activate the conda environment, open the python prompt and try:\n",
    "\n",
    "```\n",
    "    import vizdoom\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.init()\n",
    "```\n",
    "\n",
    "If a small graphical window opens, vizdoom installation is fine.\n",
    "\n",
    "Just a remark about the basic.cfg file. If you make a typo in a statement, there's no error message displayed. The statement is simply ignored by the parser of the file, leading to unexpected behavior. For example, in step 6, I first tried `screen_format = GRAY8  # used to be: CRCGCB` instead of `screen_format = GRAY8`. The parser can't handle comments behind a statement. The image was not single-channel grayscale, but remained 3-channel color, leading to the error `ValueError: ('Cannot warp empty image with dimensions', (0, 180, 320))`.\n",
    "\n",
    "More info on Vizdoom:\n",
    "* [vizdoom tutorial](http://vizdoom.cs.put.edu.pl/tutorial)\n",
    "* [vizdoom source code](https://github.com/mwydmuch/ViZDoom)\n",
    "\n",
    "## Training\n",
    "\n",
    "Training with 500 episodes takes about half an hour, however very much depending on type of computer and whether you use the GPU. The first couple of episodes take a lot of time, but as the agent improves, later episodes take less time to complete. Because the learned model is saved every 5 episodes, you can stop training before the 500 episodes have been done, without losing the training effort.\n",
    "\n",
    "Start tensorboard to see the loss decreasing:\n",
    "* tensorboard --logdir tensorboard_logs\n",
    "* http://localhost:6006/\n",
    "\n",
    "## Running\n",
    "\n",
    "After training the agent plays 100 episodes. Quite impressive to see the result in real-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: 100.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "Result: 93.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: 100.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "Result: 2.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -6.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -6.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -6.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 1, 0], reward: -1.0\n",
      "action: [0, 0, 1], reward: -1.0\n",
      "action: [1, 0, 0], reward: -1.0\n",
      "Result: -380.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from vizdoom import *  # Doom environment\n",
    "import random \n",
    "import time \n",
    "from skimage import transform\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\") \n",
    "    game.init()\n",
    "    \n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    actions = [left, right, shoot]\n",
    "    \n",
    "    return game, actions\n",
    "       \n",
    "def test_environment():\n",
    "    game, actions = create_environment()\n",
    "\n",
    "    episodes = 3\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            reward = game.make_action(action)\n",
    "            print(\"action: {}, reward: {}\".format(action, reward))\n",
    "            time.sleep(0.1)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()\n",
    "\n",
    "# just to playtest if the vizdoom environment works\n",
    "test_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  del sys.path[0]\n",
      "c:\\Users\\Justin\\anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\anaconda3\\envs\\doom\\lib\\site-packages\\ipykernel_launcher.py:265: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2 Total reward: 95.0 Training loss: 0.6641 Epsilon: 0.9798\n",
      "Episode: 3 Total reward: 93.0 Training loss: 164.5786 Epsilon: 0.9790\n",
      "Episode: 4 Total reward: 95.0 Training loss: 38.6239 Epsilon: 0.9785\n",
      "Model Saved\n",
      "Episode: 7 Total reward: 95.0 Training loss: 5.9613 Epsilon: 0.9587\n",
      "Episode: 8 Total reward: 94.0 Training loss: 19.2348 Epsilon: 0.9580\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 87.0 Training loss: 0.4275 Epsilon: 0.9380\n",
      "Episode: 12 Total reward: 92.0 Training loss: 1.3743 Epsilon: 0.9371\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 93.0 Training loss: 3.3141 Epsilon: 0.9090\n",
      "Episode: 18 Total reward: 92.0 Training loss: 3.3379 Epsilon: 0.8993\n",
      "Episode: 20 Total reward: 95.0 Training loss: 3.1877 Epsilon: 0.8899\n",
      "Model Saved\n",
      "Episode: 22 Total reward: 95.0 Training loss: 0.2893 Epsilon: 0.8806\n",
      "Episode: 23 Total reward: 92.0 Training loss: 3.4163 Epsilon: 0.8798\n",
      "Episode: 24 Total reward: 92.0 Training loss: 9.9500 Epsilon: 0.8791\n",
      "Model Saved\n",
      "Episode: 28 Total reward: 93.0 Training loss: 0.9509 Epsilon: 0.8527\n",
      "Episode: 30 Total reward: 2.0 Training loss: 2.0089 Epsilon: 0.8377\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 91.0 Training loss: 10.9520 Epsilon: 0.8369\n",
      "Episode: 32 Total reward: 89.0 Training loss: 3.5263 Epsilon: 0.8359\n",
      "Episode: 33 Total reward: 75.0 Training loss: 1.9003 Epsilon: 0.8342\n",
      "Episode: 34 Total reward: 95.0 Training loss: 81.7264 Epsilon: 0.8337\n",
      "Model Saved\n",
      "Episode: 36 Total reward: 93.0 Training loss: 7.1577 Epsilon: 0.8249\n",
      "Episode: 38 Total reward: 94.0 Training loss: 1.8679 Epsilon: 0.8162\n",
      "Episode: 39 Total reward: 93.0 Training loss: 0.8875 Epsilon: 0.8155\n",
      "Episode: 40 Total reward: 94.0 Training loss: 9.3591 Epsilon: 0.8150\n",
      "Model Saved\n",
      "Episode: 41 Total reward: 94.0 Training loss: 1.5755 Epsilon: 0.8144\n",
      "Episode: 43 Total reward: 66.0 Training loss: 12.9206 Epsilon: 0.8040\n",
      "Episode: 44 Total reward: 95.0 Training loss: 35.6172 Epsilon: 0.8035\n",
      "Episode: 45 Total reward: 95.0 Training loss: 2.5463 Epsilon: 0.8031\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 93.0 Training loss: 2.0107 Epsilon: 0.8024\n",
      "Episode: 47 Total reward: 95.0 Training loss: 3.9400 Epsilon: 0.8020\n",
      "Episode: 48 Total reward: 91.0 Training loss: 22.2284 Epsilon: 0.8012\n",
      "Episode: 50 Total reward: 92.0 Training loss: 13.2053 Epsilon: 0.7926\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 95.0 Training loss: 8.9031 Epsilon: 0.7921\n",
      "Episode: 52 Total reward: 95.0 Training loss: 6.7120 Epsilon: 0.7916\n",
      "Episode: 54 Total reward: 95.0 Training loss: 5.2608 Epsilon: 0.7834\n",
      "Episode: 55 Total reward: 94.0 Training loss: 5.6835 Epsilon: 0.7829\n",
      "Model Saved\n",
      "Episode: 56 Total reward: 95.0 Training loss: 4.5646 Epsilon: 0.7824\n",
      "Episode: 57 Total reward: 88.0 Training loss: 3.4654 Epsilon: 0.7814\n",
      "Episode: 58 Total reward: 94.0 Training loss: 1.8489 Epsilon: 0.7809\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 95.0 Training loss: 6.2254 Epsilon: 0.7651\n",
      "Episode: 62 Total reward: 91.0 Training loss: 5.6430 Epsilon: 0.7644\n",
      "Episode: 63 Total reward: 90.0 Training loss: 4.1418 Epsilon: 0.7636\n",
      "Episode: 64 Total reward: 94.0 Training loss: 1.3247 Epsilon: 0.7630\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 95.0 Training loss: 5.2934 Epsilon: 0.7551\n",
      "Episode: 67 Total reward: 76.0 Training loss: 115.3512 Epsilon: 0.7536\n",
      "Episode: 69 Total reward: -18.0 Training loss: 6.4684 Epsilon: 0.7393\n",
      "Episode: 70 Total reward: 22.0 Training loss: 7.7011 Epsilon: 0.7347\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 95.0 Training loss: 4.7756 Epsilon: 0.7342\n",
      "Episode: 72 Total reward: 8.0 Training loss: 5.4871 Epsilon: 0.7286\n",
      "Episode: 73 Total reward: 94.0 Training loss: 6.8788 Epsilon: 0.7281\n",
      "Episode: 74 Total reward: 95.0 Training loss: 13.4587 Epsilon: 0.7277\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 74.0 Training loss: 12.3015 Epsilon: 0.7190\n",
      "Episode: 77 Total reward: 94.0 Training loss: 37.9420 Epsilon: 0.7185\n",
      "Episode: 78 Total reward: 95.0 Training loss: 8.0288 Epsilon: 0.7180\n",
      "Episode: 79 Total reward: 94.0 Training loss: 6.9990 Epsilon: 0.7175\n",
      "Episode: 80 Total reward: 92.0 Training loss: 13.0281 Epsilon: 0.7169\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 94.0 Training loss: 6.9306 Epsilon: 0.7164\n",
      "Episode: 82 Total reward: 76.0 Training loss: 32.9138 Epsilon: 0.7150\n",
      "Episode: 83 Total reward: 92.0 Training loss: 5.9170 Epsilon: 0.7144\n",
      "Model Saved\n",
      "Episode: 87 Total reward: 38.0 Training loss: 5.3870 Epsilon: 0.6899\n",
      "Episode: 89 Total reward: 24.0 Training loss: 4.4010 Epsilon: 0.6790\n",
      "Episode: 90 Total reward: 95.0 Training loss: 7.2116 Epsilon: 0.6786\n",
      "Model Saved\n",
      "Episode: 92 Total reward: 95.0 Training loss: 5.5401 Epsilon: 0.6716\n",
      "Model Saved\n",
      "Episode: 96 Total reward: 48.0 Training loss: 5.1609 Epsilon: 0.6493\n",
      "Episode: 97 Total reward: 93.0 Training loss: 4.4265 Epsilon: 0.6487\n",
      "Episode: 98 Total reward: 95.0 Training loss: 13.5706 Epsilon: 0.6484\n",
      "Episode: 99 Total reward: 95.0 Training loss: 9.0223 Epsilon: 0.6480\n",
      "Episode: 100 Total reward: 93.0 Training loss: 13.8919 Epsilon: 0.6475\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 92.0 Training loss: 12.5029 Epsilon: 0.6469\n",
      "Episode: 103 Total reward: 95.0 Training loss: 9.5909 Epsilon: 0.6402\n",
      "Episode: 105 Total reward: 95.0 Training loss: 6.7401 Epsilon: 0.6335\n",
      "Model Saved\n",
      "Episode: 106 Total reward: 95.0 Training loss: 4.9761 Epsilon: 0.6332\n",
      "Episode: 107 Total reward: 95.0 Training loss: 3.3717 Epsilon: 0.6328\n",
      "Episode: 108 Total reward: 95.0 Training loss: 5.9851 Epsilon: 0.6324\n",
      "Episode: 109 Total reward: 95.0 Training loss: 4.5762 Epsilon: 0.6320\n",
      "Episode: 110 Total reward: 91.0 Training loss: 7.6306 Epsilon: 0.6314\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 66.0 Training loss: 9.7573 Epsilon: 0.6296\n",
      "Episode: 112 Total reward: 95.0 Training loss: 11.1561 Epsilon: 0.6292\n",
      "Episode: 113 Total reward: 94.0 Training loss: 7.5228 Epsilon: 0.6288\n",
      "Episode: 114 Total reward: -25.0 Training loss: 6.8017 Epsilon: 0.6228\n"
     ]
    }
   ],
   "source": [
    "# normalize and resize the frame\n",
    "def preprocess_frame(frame):\n",
    "    # greyscale frame already done in basic.cfg: \"screen_format = GRAY8\"    \n",
    "    cropped_frame = frame[30:-10,30:-30]  # crop the screen (remove the roof because it contains no information)\n",
    "    normalized_frame = cropped_frame/255.0  # normalize pixel values\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])  # resize the frame to shape (84, 84)\n",
    "    \n",
    "    return preprocessed_frame\n",
    "\n",
    "\n",
    "# initialize deque with zero images; one array for each image\n",
    "stack_size = 4 # We stack 4 frames\n",
    "stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # clear stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # stack the frames; result is a tensor\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)  # resulting shape (84, 84, 4)\n",
    "        \n",
    "    else:\n",
    "        # append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # build the stacked state\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames\n",
    "\n",
    "\n",
    "game, possible_actions = create_environment()\n",
    "state_size = [84,84,4]  # input is a stack of 4 frames hence 84x84x4 (width, height, stackinglayers) \n",
    "action_size = game.get_available_buttons_size()  # 3 possible actions: left, right, shoot\n",
    "\n",
    "# learning parameters\n",
    "learning_rate =  0.0002 \n",
    "n_episodes = 500   \n",
    "max_steps = 100  # max possible steps in an episode\n",
    "batch_size = 64             \n",
    "gamma = 0.95  # discounting rate\n",
    "\n",
    "# exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# memory hyperparameters\n",
    "pretrain_length = batch_size   # number of experiences stored in the memory when initialized for the first time\n",
    "memory_size = 1000000          # number of experiences the memory can keep\n",
    "\n",
    "training = True\n",
    "\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # create the placeholders\n",
    "            # *state_size means take each element of state_size in a tuple; like if we wrote [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # remember that target_Q is the R(s,a) + y * max Q_hat(s', a')  (Q_hat is the estimated Q)\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\" First convnet: CNN, BatchNormalization, ELU \"\"\"\n",
    "            # input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\" Second convnet: CNN, BatchNormalization, ELU \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\" Third convnet: CNN, BatchNormalization, ELU \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is the predicted Q-value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # the loss is the MSE of predicted Q_values and the Q_target\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            \n",
    "# instantiate the DQNetwork\n",
    "tf.reset_default_graph()\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]\n",
    "    \n",
    "# instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    if i == 0:  # the first step\n",
    "        state = game.get_state().screen_buffer  # First we need a state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)  # random action\n",
    "    reward = game.make_action(action)\n",
    "    done = game.is_episode_finished()  # look if the episode is finished\n",
    "    \n",
    "    if done:  # we're dead\n",
    "        next_state = np.zeros(state.shape)  # we finished the episode\n",
    "        memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "        \n",
    "        game.new_episode()  # start a new episode\n",
    "        state = game.get_state().screen_buffer  # first we need a state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  # Stack the frames\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer  # get the next state\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        \n",
    "        memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "writer = tf.summary.FileWriter(\"./tensorboard_logs/dqn\")  # setup TensorBoard Writer\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "write_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# the Q-learning part\n",
    "# choose action a from state s using epsilon greedy policy\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)  # random action (exploration)\n",
    "        \n",
    "    else:\n",
    "        # get action from Q-network (exploitation)\n",
    "        # estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)  # Take the biggest Q value (= the best action)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  # will help to save our model\n",
    "\n",
    "if training:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            step = 0            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer  # observe the first state\n",
    "            \n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)  # stack_frames() also calls preprocess()\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                decay_step +=1\n",
    "                \n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()                \n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    step = max_steps  # set step = max_steps to end the episode\n",
    "\n",
    "                    total_reward = np.sum(episode_rewards)  # get the total reward of the episode\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Epsilon: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer  # get the next state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)  # stack the frame of the next_state\n",
    "                    memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "                    state = next_state  # new state becomes current state\n",
    "\n",
    "\n",
    "                # learning part            \n",
    "                # obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # if we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # write TF summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished training the agent! Let the agent play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8076\\3767226558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossible_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_environment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtotalScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    n_episodes = 100\n",
    "    totalScore = 0\n",
    "    saver.restore(sess, \"./models/model.ckpt\")  # load the model\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        \n",
    "        done = False\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            choice = np.argmax(Qs)  # greedy policy\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break                  \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
