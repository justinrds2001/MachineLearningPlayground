{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9033ddd8",
   "metadata": {},
   "source": [
    "# Neuroevolution for Gymnasium Cartpole\n",
    "\n",
    "This program trains a neural network using neuroevolution to tackle Gymnasium Cartpole, a reinforcement learning problem.\n",
    "\n",
    "As neuroevolution only needs feed forward through the neural network, the program only uses Numpy and not Pytorch. The program allows making an NN of any shape.\n",
    "\n",
    "The best NN is saved to disk.\n",
    "\n",
    "<img src=\"cart_pole.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Some info about the Cartpole reinforcement problem:\n",
    "* The observation space consists of `Cart Position`, `Cart Velocity`, `Pole Angle` and `Pole Angular Velocity`.\n",
    "* The action space consists of `Push cart to the left` and `Push cart to the right`. \n",
    "* Since the goal is to keep the pole upright for as long as possible, a reward is `+1` for every step taken,\n",
    "  including the termination step.\n",
    "\n",
    "The velocity that is reduced or increased by the applied action is not fixed and depends on the angle the pole is pointing. This is because the center of gravity of the pole varies the amount of energy needed to move the cart underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9756650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    activation function\n",
    "    '''\n",
    "    return np.where(x > 0, x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    convert the output to probabilities by using softmax\n",
    "    '''\n",
    "    x = np.exp(x - np.max(x))\n",
    "    x[x == 0] = 1e-15  # to avoid division by 0\n",
    "    return np.array(x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a56a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    '''\n",
    "    neural network class that interacts with an Gymnasium environment\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_units=None, copy_network=None, var=0.02):\n",
    "        if copy_network is None:  # create new NN\n",
    "            self.n_units = n_units\n",
    "            weights = []\n",
    "            biases = []\n",
    "            # initialize weights and biases\n",
    "            for i in range(len(n_units)-1):\n",
    "                weights.append(np.random.normal(loc=0, scale=1, size=(n_units[i], n_units[i+1])))\n",
    "                biases.append(np.zeros(n_units[i+1]))\n",
    "            # put weights and biases in a dictionary\n",
    "            self.params = {'weights': weights,'biases': biases}\n",
    "        else:  # copy the NN\n",
    "            self.n_units = copy_network.n_units\n",
    "            weights = []\n",
    "            biases = []\n",
    "            for layer_weights in copy_network.params['weights']:\n",
    "                weights.append(layer_weights)\n",
    "            for layer_biases in copy_network.params['biases']:\n",
    "                biases.append(layer_biases)\n",
    "            self.params = {'weights': weights, 'biases': biases}\n",
    "            # perform mutation of weights and biases\n",
    "            self.params['weights'] = [x+np.random.normal(loc=0, scale=var, size=x.shape) for x in self.params['weights']]\n",
    "            self.params['biases'] = [x+np.random.normal(loc=0, scale=var, size=x.shape) for x in self.params['biases']]\n",
    "            \n",
    "    def feed_forward(self, X):\n",
    "        weights = self.params['weights']\n",
    "        biases = self.params['biases']\n",
    "        # first propagate inputs\n",
    "        a = relu((X@weights[0]) + biases[0])\n",
    "        # then propagate through every other layer\n",
    "        for layer in range(1, len(weights)):\n",
    "            a = relu((a@weights[layer]) + biases[layer])\n",
    "        probs = softmax(a)\n",
    "        return np.argmax(probs)\n",
    "        \n",
    "    def evaluate(self, n_episodes, max_episode_length, render_env):\n",
    "        '''\n",
    "        Evaluates the performance of the NN by playing plays `n_episodes` of the Cartpole game. \n",
    "        Actions are predicted by the NN. \n",
    "        Evaluate() returns the mean reward of the `n_episodes` games to obtain a reliable evaluation.\n",
    "        \n",
    "        max_episode_length: limits the max length of an episode to max_episode_length steps\n",
    "        render_env: boolean to turn on/off rendering of the environment\n",
    "        '''\n",
    "        if render_env:\n",
    "            env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "        else:\n",
    "            env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "        env._max_episode_steps=1e20  # do not use max episode length in Gymnasium TimeLimit wrapper\n",
    "            \n",
    "        rewards = []\n",
    "        for _ in range(n_episodes):\n",
    "            observation, info = env.reset()\n",
    "            episode_reward = 0\n",
    "            for _ in range(max_episode_length):\n",
    "                if render_env:\n",
    "                    env.render()\n",
    "                observation, reward, terminated, truncated, _ = env.step(self.feed_forward(np.array(observation)))\n",
    "                assert not truncated, 'episode truncated by Gymnasium' \n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    rewards.append(episode_reward)\n",
    "                    break\n",
    "        env.close()\n",
    "\n",
    "        if len(rewards) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.array(rewards).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504b3564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a random network\n",
    "random_network = NeuralNet(n_units=(4, 16, 2))\n",
    "random_network.evaluate(n_episodes=1, max_episode_length=int(1e10), render_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6f131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAlg():\n",
    "    '''\n",
    "    handles the population of NNs    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, architecture, population_size, generations, mutation_variance, n_episodes, max_episode_length, \n",
    "                 render_env, verbose, print_every):\n",
    "        # create list of NNs\n",
    "        self.networks = [NeuralNet(architecture) for _ in range(population_size)]\n",
    "        self.best_network = NeuralNet(architecture)\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_variance = mutation_variance\n",
    "        self.fitness = []\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.render_env = render_env\n",
    "        self.verbose = verbose\n",
    "        self.print_every = print_every\n",
    "        \n",
    "    def fit(self):\n",
    "        '''\n",
    "        For every generation the following steps are performed:\n",
    "        - the performance of every member of the population is evaluated\n",
    "        - the best network is selected and its score is saved\n",
    "        - children are created that are mutations of the best network\n",
    "        - the best network survives into the next generation and children are added to the new generation\n",
    "        '''\n",
    "        for i in range(self.generations):\n",
    "            rewards = np.array([x.evaluate(self.n_episodes, self.max_episode_length, self.render_env) \n",
    "                                for x in self.networks])\n",
    "            # select the best NN\n",
    "            best_network = np.argmax(rewards)\n",
    "            # track best score per generation\n",
    "            self.fitness.append(np.max(rewards))\n",
    "            # create child NNs that are mutations of the best NN\n",
    "            new_networks = [NeuralNet(copy_network=self.networks[best_network], var=self.mutation_variance) \n",
    "                            for _ in range(self.population_size-1)]\n",
    "            # only the best NN survives + add the children\n",
    "            self.networks = [self.networks[best_network]] + new_networks\n",
    "            if self.verbose is True and (i % self.print_every == 0 or i == 0):\n",
    "                print(f'Generation: {i+1} | Highest Reward: {rewards.max().round(1)} | Average Reward: '\n",
    "                      f'{rewards.mean().round(1)}')\n",
    "        # save the best network for playing\n",
    "        self.best_network = self.networks[best_network]\n",
    "        with open('best_NN.pkl', 'wb') as f:\n",
    "            pickle.dump(self.best_network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23a9377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1 | Highest Reward: 1084.4 | Average Reward: 47.4\n",
      "Generation: 2 | Highest Reward: 5368.5 | Average Reward: 257.8\n",
      "Generation: 3 | Highest Reward: 3798.9 | Average Reward: 592.4\n",
      "Generation: 4 | Highest Reward: 3621.3 | Average Reward: 438.0\n",
      "Generation: 5 | Highest Reward: 4260.6 | Average Reward: 471.5\n",
      "Finished in 226.222 seconds\n"
     ]
    }
   ],
   "source": [
    "# train a population of NNs\n",
    "start_time = time()\n",
    "# hyperparameter values determined by means of trial and error!!\n",
    "genetic_population = GeneticAlg(architecture=(4, 16, 2),\n",
    "                         population_size=64, \n",
    "                         generations=5,\n",
    "                         mutation_variance=0.1,\n",
    "                         n_episodes=15, \n",
    "                         max_episode_length=10000,\n",
    "                         render_env=False,\n",
    "                         verbose=True,\n",
    "                         print_every=1)\n",
    "genetic_population.fit()\n",
    "print(f'Finished in {round(time() - start_time, 3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03cbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best network from disk\n",
    "with open('best_NN.pkl', 'rb') as f:\n",
    "    genetic_population.best_network = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aeb415c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Justin\\MachineLearningPlayground\\ML4\\Workshop4_ML4\\Neuroevolution for Gymnasium Cartpole.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# play an episode using the best network\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m genetic_population\u001b[39m.\u001b[39;49mbest_network\u001b[39m.\u001b[39;49mevaluate(n_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, max_episode_length\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(\u001b[39m1e10\u001b[39;49m), render_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Justin\\MachineLearningPlayground\\ML4\\Workshop4_ML4\\Neuroevolution for Gymnasium Cartpole.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mif\u001b[39;00m render_env:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m observation, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(np\u001b[39m.\u001b[39;49marray(observation)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated, \u001b[39m'\u001b[39m\u001b[39mepisode truncated by Gymnasium\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Justin/MachineLearningPlayground/ML4/Workshop4_ML4/Neuroevolution%20for%20Gymnasium%20Cartpole.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\envs\\snake_game\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\envs\\snake_game\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\envs\\snake_game\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\envs\\snake_game\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:190\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    187\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\Justin\\anaconda3\\envs\\snake_game\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:302\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    301\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 302\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    303\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    305\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# play an episode using the best network\n",
    "genetic_population.best_network.evaluate(n_episodes=1, max_episode_length=int(1e10), render_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed8e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
