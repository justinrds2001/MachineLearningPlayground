{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solarboat custom OpenAI Gym environment\n",
    "\n",
    "This notebook shows how to create an OpenAI Gym environment from a solarboat physics model. Two versions are created. A version with a discrete state and action space. A version with a continuous state and action space. Your task: create an agent that learns.\n",
    "\n",
    "\n",
    "## Solar boat info\n",
    "\n",
    "Available sensor data:\n",
    "- RPM of the motor\n",
    "- position from GPS\n",
    "- battery current to the motor\n",
    "- solar power current\n",
    "- battery voltage\n",
    "\n",
    "Foreseen sensor data:\n",
    "- pitot tube to detect water current\n",
    "- wind speed\n",
    "- sun intensity\n",
    "\n",
    "Sensor data determines whether the RL model can be a MDP or POMDP.\n",
    "\n",
    "Actions that the captain can perform to influence the boat behavior: direction, turn on/off pump, cooling (is automatic), emergency stop, motor power\n",
    "\n",
    "\n",
    "## Some typical values of the Avans solar boats A1 and A2:\n",
    "\n",
    "- boat speed of A1: 12 km/h\n",
    "- expected boat speed of A2: 20 km/h\n",
    "- expected mass of A2: 120-150 kg without captain of 70 kg\n",
    "- solar boat team expects 1400 W average power\n",
    "- max. power motor?\n",
    "\n",
    "- solar panel power (actually available for boat propulsion):\n",
    "- max (according to match rules) and actual solar panel surface: 6 m^2\n",
    "- 1400 W average power (== 233 W/m^2) (== 23% efficiency, assuming sun power on a sunny day of 1 kW/m^2)\n",
    "- motor efficiency: 80%\n",
    "- drive train efficiency: 90%\n",
    "- propeller efficiency: 40%\n",
    "- ==> actual propulsion power from solar C_solar = 1 * 0.23 * 6 * 0.8 * 0.9 * 0.4 = 0.40 kW (energy after 300 sec = 120 kJ)\n",
    "\n",
    "\n",
    "battery:\n",
    "- max (according to match rules) and actual capacity battery of solar boat A2: 1.5 kWh\n",
    "- solar boat battery is allowed to be fully charged at start\n",
    "- ==> battery start energy E_battery_start = 1500 * 3600 = 5400 kJ\n",
    "\n",
    "boat drag:\n",
    "- typical speed v: 5 m/s or 18 km/h\n",
    "- ==> C_drag = 10 means \"F(v) = 250 Newton at v = 5 m/s\" or \"25 kg at 18 km/h\" or 1250 Watt dissipation\n",
    "\n",
    "\n",
    "## Physics model of the environment\n",
    "\n",
    "Model: battery, solar panel, electric motor, boat\n",
    "\n",
    "Time t and speed v are variable.\n",
    "\n",
    "Assumptions (for the analytical approach):\n",
    "- drag proportional to the square of speed, assuming higher speeds (no linear component)\n",
    "- boat has mass 0 (no kinetic energy) -> so instantaneous velocity change\n",
    "- C_solar is the actual propulsion power (so includes efficiency of complete drive train, motor, propeller)\n",
    "- C_solar is constant (independent of t, v and sun power) (e.g. no effects of motor temperature)\n",
    "- sun power is constant => P_charge is constant\n",
    "- battery has no power max\n",
    "- battery has no max capacity E_battery_capacity = infinity\n",
    "- battery has an initial charge level of E_battery_start\n",
    "- motor has no power max\n",
    "\n",
    "`\n",
    "x(t) = v * t\n",
    "F(v) = C_drag * v**2\n",
    "E_drag(t,v) = F * x = C_drag * x**3 / t**2\n",
    "P_drag(v) = E_drag(t,v) / t = F(v) * v = C_drag * v**3\n",
    "E_charge(t) = C_solar * t\n",
    "P_charge = E_charge(t) / t = C_solar\n",
    "E(t,v) = E_battery_start + E_charge(t) - E_drag(t,v)\n",
    "       = E_battery_start + C_solar * t - C_drag * v**3 * t\n",
    "       = E_battery_start + C_solar * t - C_drag * x(t)**3 / t**2\n",
    "`\n",
    "\n",
    "minimal time (== max speed): if at the finish E(t,v) = 0 and constant speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below an implementation of this physics model for the solarboat with a discrete state and action space. Questions to ask yourself:\n",
    "* How big is the state space?\n",
    "* How big is the action space?\n",
    "* Does the reward function make sense?\n",
    "* Is there a way to make the problem easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solarboat_sprint_env\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"solarboat_sprint_env\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "class SolarBoatSprintEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.max_position = 2000.0  # min_position = 0.0, start_position = 0.0\n",
    "        self.max_energy = 1800.0  # goal_energy = 0.0, min_energy = 0.0, start_energy = max_energy\n",
    "        self.goal_position = self.max_position\n",
    "        self.max_speed = 10  # min_speed = 0\n",
    "        self.n_speed_intervals = 1  # 4 means speed interval of 0.25 m/s\n",
    "        self.solar_coef = 1.0\n",
    "        self.drag_coef = 25.0\n",
    "        self.timestep = 0\n",
    "        self.max_time = 1000\n",
    "\n",
    "        self.low = np.array([0.0, 0.0])  # [min_position, min_energy]\n",
    "        self.high = np.array([self.max_position, self.max_energy])\n",
    "\n",
    "        self.viewer = None\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.max_speed * self.n_speed_intervals + 1)\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        self.timestep += 1\n",
    "        position, energy = self.state\n",
    "        velocity = action / self.n_speed_intervals\n",
    "        energy += self.solar_coef - (self.drag_coef * velocity ** 3) / 1000\n",
    "        #if energy > 0.0:  # position can only change if energy left\n",
    "        position += velocity  # assume time steps of 1 second\n",
    "        log.info(\"time: %d, position: %0f, energy: %0f, velocity: %0f\", self.timestep, position, energy, velocity)\n",
    "        energy = np.clip(energy, 0.0, self.max_energy)\n",
    "        position = np.clip(position, 0.0, self.max_position)\n",
    "\n",
    "        # calculate reward\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        reward += -1.0  # for every increase of t_\n",
    "        if position == self.goal_position:  # reached the finish\n",
    "            done = True\n",
    "            reward += 1000  # - energy  # punished for remaining energy\n",
    "            log.debug(\"reached finish, remaining energy is %0f\", energy)\n",
    "        if energy == 0.0 and position < self.goal_position:\n",
    "            done = True  # can continue on solar, but must be punished in this step for trying this speed without sufficient energy\n",
    "            reward += -10000  # run out of energy before reaching the finish\n",
    "            log.debug(\"run out of energy, position is %0f\", position)\n",
    "        if self.timestep == self.max_time:\n",
    "            done = True\n",
    "            log.debug(\"run out of time, position is %0f, remaining energy is %0f\", position, energy)\n",
    "\n",
    "        self.state = (position, energy)\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        log.info(\"reset called\")\n",
    "        self.state = (0.0, self.max_energy)  # (start_position, start_energy)\n",
    "        self.timestep = 0\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        log.info(\"time: %d, position: %0f, energy: %0f\", self.timestep, self.state[0], self.state[1])\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small demo how to use SolarBoatSprintEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: SolarBoatSprintEnv()])\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    speed = 5\n",
    "    obs, reward, done, info = env.step([speed])\n",
    "    print(\"reward: \", reward)\n",
    "    time.sleep(0.1)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below an implementation of this physics model for the solarboat with a discrete state and action space. Questions to ask yourself:\n",
    "* How big is the state space?\n",
    "* How big is the action space?\n",
    "* Does the reward function make sense?\n",
    "* Is there a way to make the problem easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solarboat_sprint_continuous_env\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"solarboat_sprint_continuous_env\")\n",
    "log.setLevel(logging.INFO)\n",
    "log.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "class SolarBoatSprintContinuousEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_pos = 0.0\n",
    "        self.max_pos = 2000.0\n",
    "        self.start_pos = self.min_pos\n",
    "        self.goal_pos = self.max_pos\n",
    "\n",
    "        self.min_speed = 0\n",
    "        self.max_speed = 10\n",
    "        # action space for the ddpg algorithm must be symmetric\n",
    "        self.middle_speed = (self.max_speed - self.min_speed) / 2\n",
    "\n",
    "        self.min_energy = 0.0\n",
    "        self.max_energy = 1800.0\n",
    "        self.start_energy = self.max_energy\n",
    "        self.goal_energy = self.min_energy\n",
    "\n",
    "        self.solar_coef = 1.0\n",
    "        self.drag_coef = 25.0\n",
    "        self.time_step = 0\n",
    "        self.max_time = 1000\n",
    "\n",
    "        self.low = np.array([self.min_pos, self.min_energy])\n",
    "        self.high = np.array([self.max_pos, self.max_energy])\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "\n",
    "        self.low = np.array([self.min_speed - self.middle_speed])\n",
    "        self.high = np.array([self.max_speed - self.middle_speed])\n",
    "        self.action_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "\n",
    "        self.episode_reward = None\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "        self.old_state = None\n",
    "        self.np_random = None\n",
    "        self.boat_trans = None\n",
    "        self.velocity = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        self.time_step += 1\n",
    "        pos, energy = self.state\n",
    "        velocity = action[0] + self.middle_speed\n",
    "        energy += self.solar_coef - (self.drag_coef * velocity ** 3) / 1000\n",
    "        #if energy > 0.0:  # position can only change if energy left\n",
    "        pos += velocity  # assume time steps of 1 second\n",
    "        log.debug(\"(before clipping) time: %d, position: %.0f, energy: %.0f, velocity: %.1f\",\n",
    "                  self.time_step, pos, energy, velocity)\n",
    "        pos = np.clip(pos, self.min_pos, self.max_pos)\n",
    "        energy = np.clip(energy, self.min_energy, self.max_energy)\n",
    "\n",
    "        # calculate reward\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        reward += -1.0  # for every increase of time_step\n",
    "        if pos == self.goal_pos:  # reached the finish\n",
    "            done = True\n",
    "            reward += 1000  # - (energy - self.goal_energy)  # punished for remaining energy\n",
    "            log.info(\"reached finish, remaining energy is %.0f, episode_reward is %.0f\",\n",
    "                     energy, self.episode_reward + reward)\n",
    "        if energy == 0.0:  # and pos < self.goal_pos:\n",
    "            done = True  # can continue on solar, but must be punished in this step for trying this speed without sufficient energy\n",
    "            reward += -10000  # run out of energy before reaching the finish\n",
    "            log.info(\"run out of energy, position is %.0f, episode_reward is %.0f\",\n",
    "                     pos, self.episode_reward + reward)\n",
    "        if self.time_step == self.max_time:\n",
    "            done = True\n",
    "            log.info(\"run out of time, position is %.0f, remaining energy is %.0f, episode_reward is %.0f\",\n",
    "                     pos, energy, self.episode_reward + reward)\n",
    "\n",
    "        self.old_state = self.state\n",
    "        self.state = (pos, energy)\n",
    "        self.episode_reward += reward\n",
    "        return self.state, reward, done, {\"v (non-clipped)\": velocity}\n",
    "\n",
    "    def reset(self):\n",
    "        log.debug(\"reset called\")\n",
    "        self.state = np.array((self.start_pos, self.max_energy))\n",
    "        self.time_step = 0\n",
    "        self.episode_reward = 0.0\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        speed = self.state[0] - self.old_state[0]\n",
    "        log.info(\"time: %d, position: %.0f, energy: %.0f, speed: %.1f\",\n",
    "                 self.time_step, self.state[0], self.state[1], speed)\n",
    "\n",
    "        # to allow same code as slalom continuous env\n",
    "        self.max_pos_x = self.max_pos\n",
    "        self.min_pos_x = self.min_pos\n",
    "        self.goal_pos_x = self.max_pos_x\n",
    "        self.max_pos_y = 500.0\n",
    "        self.min_pos_y = 0.0\n",
    "\n",
    "        world_width = self.max_pos_x - self.min_pos_x\n",
    "        world_height = self.max_pos_y - self.min_pos_y\n",
    "        screen_width = 600\n",
    "        scale = screen_width / world_width\n",
    "        screen_height = int(world_height * scale)\n",
    "        margin = 50\n",
    "        screen_width += 2 * margin\n",
    "        screen_height += 2 * margin\n",
    "\n",
    "        def scl(val):\n",
    "            return val * scale + margin\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            boat_width = 40\n",
    "            boat_height = 20\n",
    "            l, r, t, b = -boat_width * 1.5, -boat_width * 0.5, boat_height / 2, -boat_height / 2\n",
    "            boat = rendering.make_polygon([(l, b), (l, t), (r, t), (r + boat_width * 0.5, (t + b) / 2), (r, b)])\n",
    "            self.viewer.add_geom(boat)\n",
    "            self.boat_trans = rendering.Transform()\n",
    "            boat.add_attr(self.boat_trans)\n",
    "\n",
    "            l, r = scl(self.min_pos_x), scl(self.max_pos_x)\n",
    "            t, b = scl(self.max_pos_y), scl(self.min_pos_y)\n",
    "            track = rendering.make_polyline([(l, b), (l, t), (r, t), (r, b), (l, b)])\n",
    "            self.viewer.add_geom(track)\n",
    "\n",
    "            flag_x = scl(self.goal_pos_x)\n",
    "            flag_y1 = scl(self.max_pos_y)\n",
    "            flag_y2 = flag_y1 + 50\n",
    "            flagpole = rendering.PolyLine([(flag_x, flag_y1), (flag_x, flag_y2)], False)\n",
    "            flagpole.set_linewidth(4)\n",
    "            self.viewer.add_geom(flagpole)\n",
    "            flag = rendering.FilledPolygon([(flag_x, flag_y2), (flag_x, flag_y2 - 10), (flag_x + 25, flag_y2 - 5)])\n",
    "            flag.set_color(1, 0, 0)\n",
    "            self.viewer.add_geom(flag)\n",
    "\n",
    "        self.boat_trans.set_translation(scl(self.state[0] - self.min_pos_x), scl((self.max_pos_y - self.min_pos_y) / 2))\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous state and action space the DDPG and PPO2 algorithms are a suitable choice.\n",
    "\n",
    "DDPG can be thought of as being deep Q-learning for continuous action spaces.\n",
    "\n",
    "`    \n",
    " n_actions = env.action_space.shape[-1]\n",
    " param_noise = None\n",
    " action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    " from stable_baselines.ddpg.policies import MlpPolicy\n",
    " model = DDPG(MlpPolicy, env, verbose=1, param_noise=param_noise, action_noise=action_noise, \n",
    "              tensorboard_log=\"tensor_boards/solarboat_actorcritic_agent/\")\n",
    "`\n",
    "\n",
    "The Proximal Policy Optimization actor-critic algorithm combines ideas from A2C (having multiple workers) and TRPO (it uses a trust region to improve the actor).\n",
    "\n",
    "`    \n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "model = PPO2(MlpPolicy, env, learning_rate=0.005, tensorboard_log=\"tensor_boards/solarboat_actorcritic_agent/\")`\n",
    "\n",
    "Small demo how to use SolarBoatSprintContinuousEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: SolarBoatSprintContinuousEnv()])\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    speed = [[5.0]]\n",
    "    obs, reward, done, info = env.step(speed)\n",
    "    env.render()\n",
    "    #print(\"reward: \", reward)\n",
    "    time.sleep(0.2)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
