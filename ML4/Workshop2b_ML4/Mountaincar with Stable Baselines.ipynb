{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Stable Baselines RL library\n",
    "\n",
    "[Documentation of Stable Baselines](https://stable-baselines3.readthedocs.io/en/master/index.html).\n",
    "\n",
    "## Installation of Stable Baselines\n",
    "\n",
    "* `conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia`\n",
    "* `conda install gymnasium numpy matplotlib stable-baselines3[extra] tensorboard pip jupyter notebook -c conda-forge`\n",
    "\n",
    "## Test Stable Baselines using Gymnasium Mountain Car\n",
    "\n",
    "<img src=\"mountain-car-v0.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Goal: drive up the mountain on the right. However, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Let's have a look at the [documentation](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) to understand the environment.\n",
    "\n",
    "Note that \"car position\" is along the x-axis (1-dimensional).\n",
    "\n",
    "What can we state about the problem?\n",
    "* To be able to reach the mountain top at the right, the car needs to swing to the left. This makes it not so obvious what a good reward function would be\n",
    "\n",
    "### 1st attempt: DQN\n",
    "* Let's try DQN.\n",
    "* Let's try 100 episodes. From the documentation, we understand that an episode takes maximally 200 steps. So we need 20000 steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training, now use the trained model and render the env\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n"
     ]
    }
   ],
   "source": [
    "# mountaincar_dqn_agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "env.metadata['render_fps'] = 200\n",
    "\n",
    "training = False\n",
    "if training:\n",
    "    # mountaincar does not give good results with DQN default settings,\n",
    "    # so use tuned settings from SB3 Zoo: rl-baselines3-zoo\\hyperparams\\dqn.yml\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, device=\"cpu\", gamma=0.98, learning_rate=0.005, buffer_size=10000, exploration_fraction=0.2, exploration_final_eps=0.07, exploration_initial_eps=1.0, train_freq=16, gradient_steps=8, batch_size=128, learning_starts=1000, target_update_interval=600, _init_setup_model=True, policy_kwargs=dict(net_arch=[256, 256]), tensorboard_log=\"tensorboard_logs/mountaincar_dqn_agent/\")\n",
    "    model.learn(total_timesteps=20000)#20000)\n",
    "    model.save(\"learned_models/dqn_mountaincar\")\n",
    "else:\n",
    "    # model = DQN.load(\"learned_models/dqn_mountaincar\")\n",
    "    model = DQN.load(\"learned_models/dqn_mountaincar_400000steps\")\n",
    "\n",
    "print(\"finished training, now use the trained model and render the env\")\n",
    "\n",
    "env.env.env.env.render_mode = \"human\" # 'env.env.env.env.' due to all wrapping done internally by gymnasium\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_steps = 0\n",
    "    while not done:\n",
    "        action, state = model.predict(obs)  # greedy policy\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_steps += 1\n",
    "        env.render()\n",
    "\n",
    "    result = 'Success!' if episode_steps < 200 else 'Failure!'\n",
    "    print(result)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes 1 minute to train and the result is shown below. An unsuccessful episode gives an episode reward of -200. So we must conclude that the agent never managed to get a success.\n",
    "  <img src=\"training_result_1st_attempt.png\" alt=\"drawing\" width=\"250\"/>\n",
    "  <img src=\"dqn_mountaincar_20000steps.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "  \n",
    "Options to try is to increase exploration:\n",
    "* We noted that none of the 100 episodes was successful. So the agent had really no single possibility to learn. If only we were so lucky that only one episode is successful, there's at least something to learn for the agent -->> increase the amount of exploration. Let's look at the [documentation](https://stable-baselines3.readthedocs.io/en/master/index.html) how to do this. There are several ways to do this. We could set `exploration_fraction=0.2` to `exploration_fraction=0.3`. This is the fraction of the total number of training steps over which epsilon is decreased from its max to its min value.\n",
    "\n",
    "\n",
    "### 2nd attempt: longer learning\n",
    "* Let's try to not be intelligent. We can also simply train for a longer time, hoping that we will be lucky once that the car reaches the top. We try 400k steps.\n",
    "* Note that in Stable Baselines time spent in exploration is proportional to the number of steps. So more steps means longer exploration.\n",
    "* It takes 10 minutes and the result is shown below. Some success! Quite often the car reaches the top.\n",
    "  <img src=\"dqn_mountaincar_400000steps.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "* Set `Training = False` and let's try to play a couple of time (load the saved model \"dqn_mountaincar_400000steps\"). As you can see brute force worked out quite well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd attempt: tweaking the reward function\n",
    "\n",
    "Let's try a different approach and try to tweak the reward function of the mountaincar environment. Remember that defining subgoals helps learning, but possibly removes the guarantee of optimality.\n",
    "\n",
    "* make a copy of source code of the official Gymnasium Mountaincar, which you can find in `...\\envs\\<your env>\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py`\n",
    "* the max episode length of 200 is enforced by the gym environment wrapper, not by the environment itself. So I've added code for this by hand (see below for the code).\n",
    "\n",
    "**Original reward function**:\n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0\n",
    "    ...`\n",
    "\n",
    "Note that looking at this code, there's no reward for reaching the finish. There's only a reward of -1 per step. Can you explain why this reward function work anyway?\n",
    "\n",
    "**Modified reward function (1st attempt)**:\n",
    "\n",
    "To be able to reach the mountain top at the right, the car needs to swing to the left. This makes it not so obvious what a good reward function would be. \n",
    "\n",
    "The more the car manages to go to the right, the better it is, so let's reward it when the car is far to the right.\n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0 + position  # the more to the right the higher the reward\n",
    "    if position >= 0.5:  # bonus if finish is reached\n",
    "        reward = 1\n",
    "    ...`\n",
    "\n",
    "What do you think will happen?\n",
    "\n",
    "Result:\n",
    "<img src=\"dqn_mymountaincar_reward_attempt1_250k_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Can you explain it?\n",
    "\n",
    "\n",
    "**Modified reward function (2nd attempt)**:\n",
    "\n",
    "Instead of continually rewarding the agent trying to be as to the right as possible, let's only reward it when it breaks the record of begin furthest to the right. Also important to note that it is not a problem if the car swings very far to the left. The car does not \"die\". \n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0\n",
    "    if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        self.max_reached_position = position\n",
    "        reward = 5.0\n",
    "    ...`\n",
    "\n",
    "<img src=\"dqn_mymountaincar_reward_attempt2_200k_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "The results are quite good. Some discussion, hypothetically, without proof:\n",
    "* We were warned to be careful with defining subgoals. This is a subgoal\n",
    "* What will give maximum reward? \n",
    "  * If with every swing to the right, we just go a little further than the previous time, the reward is every time 5.\n",
    "  * So actually a slowly learning car will have more reward than a fast learning car, because a fast learning car has quickly reached a high max_reached_position, which will not often be exceeded any more.\n",
    "  * There's no incentive for the car to reach the finish! The longer it keeps driving, the better the reward!\n",
    "  * This all has very much to do with the time limit of 200 steps! Slow learning would work perfectly if there were no time limit.\n",
    " \n",
    "**Modified reward function (3rd and last attempt)**:\n",
    "\n",
    " Add a finish bonus that exceeds the cumulative max_reached position bonus: 5000\n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0\n",
    "    if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        self.max_reached_position = position\n",
    "        reward = 5.0\n",
    "    if position >= 0.5:  # bonus if finish is reached\n",
    "        reward = 5000.0\n",
    "    ...`\n",
    "\n",
    "<img src=\"dqn_mymountaincar_reward_attempt3_200k_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Setting `Training = False` shows quite okay results. Is this due to the reward function, or is the reward function actually not very different from the original one, and is it simply because we've trained for 200k steps? Just from eye-sight, the original reward function seems to be even slightly better, but it was trained for 400k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "http://incompleteideas.net/MountainCar/MountainCar1.cp\n",
    "permalink: https://perma.cc/6Z2N-PFWC\n",
    "\"\"\"\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "class MyMountainCarEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically\n",
    "    at the bottom of a sinusoidal valley, with the only possible actions being the accelerations\n",
    "    that can be applied to the car in either direction. The goal of the MDP is to strategically\n",
    "    accelerate the car to reach the goal state on top of the right hill. There are two versions\n",
    "    of the mountain car domain in gymnasium: one with discrete actions and one with continuous.\n",
    "    This version is the one with discrete actions.\n",
    "\n",
    "    This MDP first appeared in [Andrew Moore's PhD Thesis (1990)](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf)\n",
    "\n",
    "    ```\n",
    "    @TECHREPORT{Moore90efficientmemory-based,\n",
    "        author = {Andrew William Moore},\n",
    "        title = {Efficient Memory-based Learning for Robot Control},\n",
    "        institution = {University of Cambridge},\n",
    "        year = {1990}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "\n",
    "    | Num | Observation                          | Min   | Max  | Unit         |\n",
    "    |-----|--------------------------------------|-------|------|--------------|\n",
    "    | 0   | position of the car along the x-axis | -1.2  | 0.6  | position (m) |\n",
    "    | 1   | velocity of the car                  | -0.07 | 0.07 | velocity (v) |\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    There are 3 discrete deterministic actions:\n",
    "\n",
    "    - 0: Accelerate to the left\n",
    "    - 1: Don't accelerate\n",
    "    - 2: Accelerate to the right\n",
    "\n",
    "    ## Transition Dynamics:\n",
    "\n",
    "    Given an action, the mountain car follows the following transition dynamics:\n",
    "\n",
    "    *velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
    "\n",
    "    *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>*\n",
    "\n",
    "    where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic with the velocity set to 0\n",
    "    upon collision with the wall. The position is clipped to the range `[-1.2, 0.6]` and\n",
    "    velocity is clipped to the range `[-0.07, 0.07]`.\n",
    "\n",
    "    ## Reward:\n",
    "\n",
    "    The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is\n",
    "    penalised with a reward of -1 for each timestep.\n",
    "\n",
    "    ## Starting State\n",
    "\n",
    "    The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*.\n",
    "    The starting velocity of the car is always assigned to 0.\n",
    "\n",
    "    ## Episode End\n",
    "\n",
    "    The episode ends if either of the following happens:\n",
    "    1. Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "    2. Truncation: The length of the episode is 200.\n",
    "\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    ```python\n",
    "    import gymnasium as gym\n",
    "    gym.make('MountainCar-v0')\n",
    "    ```\n",
    "\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine\n",
    "    the new random state.\n",
    "\n",
    "    ## Version History\n",
    "\n",
    "    * v0: Initial versions release (1.0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 200, # Erco\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):\n",
    "        self._max_episode_steps = 200  # Erco\n",
    "        self.min_position = -1.2\n",
    "        self.max_position = 0.6\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = 0.5\n",
    "        self.goal_velocity = goal_velocity\n",
    "        \n",
    "        self.max_reached_position = self.min_position  # max_reached_position not in reset(), so keep its value (Erco)\n",
    "        \n",
    "        self.force = 0.001\n",
    "        self.gravity = 0.0025\n",
    "\n",
    "        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n",
    "        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        assert self.action_space.contains(\n",
    "            action\n",
    "        ), f\"{action!r} ({type(action)}) invalid\"\n",
    "\n",
    "        position, velocity = self.state\n",
    "        velocity += (action - 1) * self.force + math.cos(3 * position) * (-self.gravity)\n",
    "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
    "        position += velocity\n",
    "        position = np.clip(position, self.min_position, self.max_position)\n",
    "        if position == self.min_position and velocity < 0:\n",
    "            velocity = 0\n",
    "\n",
    "        terminated = bool(\n",
    "            position >= self.goal_position and velocity >= self.goal_velocity\n",
    "        )\n",
    "        # RewArt shaping (Erco)\n",
    "        # attempt 0: original reward function\n",
    "        reward = -1.0  \n",
    "        #\n",
    "        # attempt 1:\n",
    "        #reward = -1.0 + position  # the more to the right the higher the reward\n",
    "        #if position >= 0.5:  # bonus if finish is reached\n",
    "        #    reward = 1\n",
    "        #\n",
    "        # attempt 2:\n",
    "        #reward = -1.0\n",
    "        #if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        #    self.max_reached_position = position\n",
    "        #    reward = 5.0\n",
    "        #\n",
    "        # attempt 3:\n",
    "        #reward = -1.0\n",
    "        #if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        #    self.max_reached_position = position\n",
    "        #    reward = 5.0\n",
    "        #if position >= 0.5:  # bonus if finish is reached\n",
    "        #    reward = 5000.0\n",
    "\n",
    "        self._elapsed_steps += 1  # Erco\n",
    "        truncated =  bool(self._elapsed_steps >= self._max_episode_steps)  # the Gymnasium wrapper limits #steps (Erco)\n",
    "\n",
    "        self.state = (position, velocity)\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self._elapsed_steps = 0  # Erco\n",
    "        \n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(options, -0.6, -0.4)\n",
    "        self.state = np.array([self.np_random.uniform(low=low, high=high), 0])\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def _height(self, xs):\n",
    "        return np.sin(3 * xs) * 0.45 + 0.55\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[classic-control]`\"\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode in \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.max_position - self.min_position\n",
    "        scale = self.screen_width / world_width\n",
    "        carwidth = 40\n",
    "        carheight = 20\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        pos = self.state[0]\n",
    "\n",
    "        xs = np.linspace(self.min_position, self.max_position, 100)\n",
    "        ys = self._height(xs)\n",
    "        xys = list(zip((xs - self.min_position) * scale, ys * scale))\n",
    "\n",
    "        pygame.draw.aalines(self.surf, points=xys, closed=False, color=(0, 0, 0))\n",
    "\n",
    "        clearance = 10\n",
    "\n",
    "        l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0\n",
    "        coords = []\n",
    "        for c in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            c = pygame.math.Vector2(c).rotate_rad(math.cos(3 * pos))\n",
    "            coords.append(\n",
    "                (\n",
    "                    c[0] + (pos - self.min_position) * scale,\n",
    "                    c[1] + clearance + self._height(pos) * scale,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        gfxdraw.aapolygon(self.surf, coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, coords, (0, 0, 0))\n",
    "\n",
    "        for c in [(carwidth / 4, 0), (-carwidth / 4, 0)]:\n",
    "            c = pygame.math.Vector2(c).rotate_rad(math.cos(3 * pos))\n",
    "            wheel = (\n",
    "                int(c[0] + (pos - self.min_position) * scale),\n",
    "                int(c[1] + clearance + self._height(pos) * scale),\n",
    "            )\n",
    "\n",
    "            gfxdraw.aacircle(\n",
    "                self.surf, wheel[0], wheel[1], int(carheight / 2.5), (128, 128, 128)\n",
    "            )\n",
    "            gfxdraw.filled_circle(\n",
    "                self.surf, wheel[0], wheel[1], int(carheight / 2.5), (128, 128, 128)\n",
    "            )\n",
    "\n",
    "        flagx = int((self.goal_position - self.min_position) * scale)\n",
    "        flagy1 = int(self._height(self.goal_position) * scale)\n",
    "        flagy2 = flagy1 + 50\n",
    "        gfxdraw.vline(self.surf, flagx, flagy1, flagy2, (0, 0, 0))\n",
    "\n",
    "        gfxdraw.aapolygon(\n",
    "            self.surf,\n",
    "            [(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)],\n",
    "            (204, 204, 0),\n",
    "        )\n",
    "        gfxdraw.filled_polygon(\n",
    "            self.surf,\n",
    "            [(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)],\n",
    "            (204, 204, 0),\n",
    "        )\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def get_keys_to_action(self):\n",
    "        # Control with left and right arrow keys.\n",
    "        return {(): 1, (276,): 0, (275,): 2, (275, 276): 1}\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training, now use the trained model and render the env\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n",
      "Failure!\n"
     ]
    }
   ],
   "source": [
    "# my_mountaincar_dqn_agent\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "#env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "env = MyMountainCarEnv(0)\n",
    "env.render_mode = \"rgb_array\"\n",
    "\n",
    "training = True\n",
    "if training:\n",
    "    # mountaincar does not give good results with DQN default settings,\n",
    "    # so use tuned settings from SB3 Zoo: rl-baselines3-zoo\\hyperparams\\dqn.yml\n",
    "    model = DQN(\"MlpPolicy\", env, device=\"cpu\", verbose=0, gamma=0.98, learning_rate=0.005, buffer_size=10000, exploration_fraction=0.2, exploration_final_eps=0.07, exploration_initial_eps=1.0, train_freq=16, gradient_steps=8, batch_size=128, learning_starts=1000, target_update_interval=600, _init_setup_model=True, policy_kwargs=dict(net_arch=[256, 256]), tensorboard_log=\"tensorboard_logs/mymountaincar_dqn_agent/\")\n",
    "    model.learn(total_timesteps=20000)\n",
    "    model.save(\"learned_models/dqn_my_mountaincar\")\n",
    "else:\n",
    "    model = DQN.load(\"learned_models/dqn_my_mountaincar\")\n",
    "\n",
    "print(\"finished training, now use the trained model and render the env\")\n",
    "\n",
    "env.render_mode = \"human\"\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_steps = 0\n",
    "    while not done:\n",
    "        action, state = model.predict(obs)  # greedy policy\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_steps += 1\n",
    "        env.render()\n",
    "\n",
    "    result = 'Success!' if episode_steps < 200 else 'Failure!'\n",
    "    print(result)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
