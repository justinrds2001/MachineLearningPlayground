{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Stable Baselines RL library\n",
    "\n",
    "## Installation\n",
    "\n",
    "<u>Correct way of using pip with anaconda</u>\n",
    "* always create an environment, to not mess up the base environment: `conda create -n <myenv> python=<python version>`\n",
    "* first try conda install: `conda install <your package>`\n",
    "* if this doesn't work, try the conda forge channel: `conda install -c conda-forge <your package>`\n",
    "* if this doesn't work, try pip in the following way:\n",
    "  * `conda install pip`\n",
    "  * `<location of anaconda>\\anaconda\\envs\\<your env>\\Scripts\\pip install <your package>` (to be sure to use the correct pip binary)\n",
    "* hwo to check the location of pip:\n",
    "  * `which pip` (linux)\n",
    "  * `where pip` (windows cmd)\n",
    "  * `Get-Command pip` (windows powershell)\n",
    "\n",
    "So installing Stable Baselines will be:\n",
    "* `conda create -n py36 python=3.6`\n",
    "* `conda activate py36`\n",
    "* `conda install pip`\n",
    "* `conda install tensorflow=1.15` (stable-baselines version 2 does not support tensorflow 2, whereas version 3 uses pytorch)\n",
    "* `<location of anaconda>\\anaconda\\envs\\<your env>\\Scripts\\pip install stable-baselines` (to be sure to use the correct pip binary)\n",
    "\n",
    "Possible installation problems:\n",
    "* Error “AttributeError: module 'gym.envs.box2d' has no attribute ‘MountainCar’”. Solution is to install the packages `swig`, `pocketsphinx` and `gym[all]`.\n",
    "\n",
    "\n",
    "## Test Stable Baselines using OpenAI Gym Mountain Car\n",
    "\n",
    "<img src=\"mountain-car-v0.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Goal: drive up the mountain on the right. However, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Let's have a look at the [documentation](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) to understand the environment.\n",
    "\n",
    "Note that \"car position\" is along the x-axis (1-dimensional).\n",
    "\n",
    "What can we state about the problem?\n",
    "* To be able to reach the mountain top at the right, the car needs to swing to the left. This makes it not so obvious what a good reward function would be\n",
    "\n",
    "### 1st attempt: DQN\n",
    "* Let's try DQN.\n",
    "* Let's try 100 episodes. From the documentation, we understand that an episode takes maximally 200 steps. So we need 20000 steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mountaincar_dqn_agent\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "training = True\n",
    "if training:\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"tensorboard_logs/mountaincar_dqn_agent/\")\n",
    "    #model = DQN(\"MlpPolicy\", env, verbose=1, learning_rate=0.0005, gamma=0.99, policy_kwargs=dict(layers=[64, 64]), tensorboard_log=\"tensorboard_logs/mountaincar_dqn_agent/\")\n",
    "    model.learn(total_timesteps=20000)\n",
    "    model.save(\"learned_models/dqn_mountaincar\")\n",
    "else:\n",
    "    model = DQN.load(\"learned_models/dqn_mountaincar\")\n",
    "    #model = DQN.load(\"learned_models/dqn_mountaincar_400000steps_sb2\")\n",
    "\n",
    "print(\"finished training, now use the trained model and render the env\")\n",
    "\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_steps = 0\n",
    "    while not done:\n",
    "        action, state = model.predict(obs)  # greedy policy\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_steps += 1\n",
    "        env.render()\n",
    "\n",
    "    result = 'Success!' if episode_steps < 200 else 'Failure!'\n",
    "    print(result)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes 1 minute to train and the result is shown below. An unsuccessful episode gives an episode reward of -200. So we must conclude that the agent never managed to get a success.\n",
    "  <img src=\"training_result_1st_attempt_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "  \n",
    "Options to try is to increase exploration:\n",
    "* We noted that none of the 100 episodes was successful. So the agent had really no single possibility to learn. If only we were so lucky that only one episode is successful, there's at least something to learn for the agent -->> increase the amount of exploration. Let's look at the [documentation](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) how to do this. There are several ways to do this. We could set `exploration_fraction=0.1` to `exploration_fraction=0.2`. This is the fraction of the total number of training steps over which epsilon is decreased from its max to its min value.\n",
    "\n",
    "\n",
    "### 2nd attempt: longer learning\n",
    "* Let's try to not be intelligent. We can also simply train for a longer time, hoping that we will be lucky once that the car reaches the top. We try 400k steps.\n",
    "* Note that in Stable Baselines time spent in exploration is proportional to the number of steps. So more steps means longer exploration.\n",
    "* It takes 10 minutes and the result is shown below. Some success! Quite often the car reaches the top.\n",
    "  <img src=\"tensorboard400000_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "* Set `Training = False` and let's try to play a couple of time (load the saved model \"dqn_mountaincar_400000steps\"). As you can see brute force worked out quite well! From the tensorboard graph, we did not expect this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd attempt: tweaking the reward function\n",
    "\n",
    "Let's try a different apprach and try to tweak the reward function of the mountaincar environment. Remember that defining subgoals helps learning, but possibly removes the guarantee of optimality.\n",
    "\n",
    "* make a copy of the [official OpenAI Gym Mountaincar](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) \n",
    "* the max episode length of 200 is enforced by the gym environment wrapper, not by the environment itself. So I've added code for this by hand (see below for the code).\n",
    "\n",
    "**Original reward function**:\n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0\n",
    "    ...`\n",
    "\n",
    "Note that looking at this code, there's no reward for reaching the finish. There's only a reward of -1 per step. Can you explain why this reward function work anyway?\n",
    "\n",
    "**Modified reward function (1st attempt)**:\n",
    "\n",
    "To be able to reach the mountain top at the right, the car needs to swing to the left. This makes it not so obvious what a good reward function would be. \n",
    "\n",
    "The more the car goes to the right, the better it is, so let's reward it when the car is far to the right.\n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0 + position  # the more to the right the higher the reward\n",
    "    if position >= 0.5:  # bonus if finish is reached\n",
    "        reward = 1\n",
    "    ...`\n",
    "\n",
    "What do you think will happen?\n",
    "\n",
    "Result:\n",
    "<img src=\"dqn_mymountaincar_reward_attempt1_250k_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Can you explain it?\n",
    "\n",
    "\n",
    "**Modified reward function (2nd attempt)**:\n",
    "\n",
    "Instead of continually rewarding the agent trying to be as to the right as possible, let's only reward it when it breaks the record of begin furthest to the right. Also important to note that it is not a problem if the car swings very far to the left. The car does not \"die\". \n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0\n",
    "    if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        self.max_reached_position = position\n",
    "        reward = 5.0\n",
    "    ...`\n",
    "\n",
    "<img src=\"dqn_mymountaincar_reward_attempt2_200k_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "The results are quite good. Some discussion, hypothetically, without proof:\n",
    "* We were warned to be careful with defining subgoals. This is a subgoal\n",
    "* What will give maximum reward? \n",
    "  * If with every swing to the right, we just go a little further than the previous time, the reward is every time 5.\n",
    "  * So actually a slowly learning car will have more reward than a fast learning car, because a fast learning car has quickly reached a high max_reached_position, which will not often be exceeded any more.\n",
    "  * There's no incentive for the car to reach the finish! The longer it keeps driving, the better the reward!\n",
    "  * This all has very much to do with the time limit of 200 steps! Slow learning would work perfectly if there were no time limit.\n",
    " \n",
    "**Modified reward function (3rd and last attempt)**:\n",
    "\n",
    " Add a finish bonus that exceeds the cumulative max_reached position bonus: 5000\n",
    "\n",
    "`step(self, action):\n",
    "    ...\n",
    "    reward = -1.0\n",
    "    if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        self.max_reached_position = position\n",
    "        reward = 5.0\n",
    "    if position >= 0.5:  # bonus if finish is reached\n",
    "        reward = 5000.0\n",
    "    ...`\n",
    "\n",
    "<img src=\"dqn_mymountaincar_reward_attempt3_200k_sb2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Setting `Training = False` shows quite okay results. Is this due to the reward function, or is the reward function actually not very different from the original one, and is it simply because we've trained for 200k steps? Just from eye-sight, the original reward function seems to be even slightly better, but it was trained for 400k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "http://incompleteideas.net/sutton/MountainCar/MountainCar1.cp\n",
    "permalink: https://perma.cc/6Z2N-PFWC\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class MyMountainCarEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The agent (a car) is started at the bottom of a valley. For any given\n",
    "        state the agent may choose to accelerate to the left, right or cease\n",
    "        any acceleration.\n",
    "    Source:\n",
    "        The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
    "    Observation:\n",
    "        Type: Box(2)\n",
    "        Num    Observation               Min            Max\n",
    "        0      Car Position              -1.2           0.6\n",
    "        1      Car Velocity              -0.07          0.07\n",
    "    Actions:\n",
    "        Type: Discrete(3)\n",
    "        Num    Action\n",
    "        0      Accelerate to the Left\n",
    "        1      Don't accelerate\n",
    "        2      Accelerate to the Right\n",
    "        Note: This does not affect the amount of velocity affected by the\n",
    "        gravitational pull acting on the car.\n",
    "    Reward:\n",
    "         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "         on top of the mountain.\n",
    "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "    Starting State:\n",
    "         The position of the car is assigned a uniform random value in\n",
    "         [-0.6 , -0.4].\n",
    "         The starting velocity of the car is always assigned to 0.\n",
    "    Episode Termination:\n",
    "         The car position is more than 0.5\n",
    "         Episode length is greater than 200\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 30\n",
    "    }\n",
    "\n",
    "    def __init__(self, goal_velocity=0):\n",
    "        self._max_episode_steps = 200  # Erco\n",
    "        self.min_position = -1.2\n",
    "        self.max_position = 0.6\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = 0.5\n",
    "        self.goal_velocity = goal_velocity\n",
    "\n",
    "        self.max_reached_position = self.min_position  # max_reached_position not in reset(), so keeps its value (Erco)\n",
    "\n",
    "        self.force = 0.001\n",
    "        self.gravity = 0.0025\n",
    "\n",
    "        self.low = np.array(\n",
    "            [self.min_position, -self.max_speed], dtype=np.float32\n",
    "        )\n",
    "        self.high = np.array(\n",
    "            [self.max_position, self.max_speed], dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            self.low, self.high, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action, type(action))\n",
    "\n",
    "        position, velocity = self.state\n",
    "        velocity += (action - 1) * self.force + math.cos(3 * position) * (-self.gravity)\n",
    "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
    "        position += velocity\n",
    "        position = np.clip(position, self.min_position, self.max_position)\n",
    "        if (position == self.min_position and velocity < 0):\n",
    "            velocity = 0\n",
    "\n",
    "        done = bool(\n",
    "            position >= self.goal_position and velocity >= self.goal_velocity\n",
    "        )\n",
    "        # Erco\n",
    "        # attempt 0: original reward function\n",
    "        #reward = -1.0  \n",
    "        #\n",
    "        # attempt 1:\n",
    "        #reward = -1.0 + position  # the more to the right the higher the reward\n",
    "        #if position >= 0.5:  # bonus if finish is reached\n",
    "        #    reward = 1\n",
    "        #\n",
    "        # attempt 2:\n",
    "        reward = -1.0\n",
    "        if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "            self.max_reached_position = position\n",
    "            reward = 5.0\n",
    "        #\n",
    "        # attempt 3:\n",
    "        #reward = -1.0\n",
    "        #if position > self.max_reached_position:  # reward when new maximally right position has been reached\n",
    "        #    self.max_reached_position = position\n",
    "        #    reward = 5.0\n",
    "        #if position >= 0.5:  # bonus if finish is reached\n",
    "        #    reward = 5000.0\n",
    "\n",
    "            \n",
    "        self._elapsed_steps += 1  # Erco\n",
    "        if self._elapsed_steps >= self._max_episode_steps:  # the OpenAI Gym wrapper limits #steps (Erco)\n",
    "            done = True  # Erco\n",
    "            \n",
    "\n",
    "        self.state = (position, velocity)\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self._elapsed_steps = 0  # Erco\n",
    "        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def _height(self, xs):\n",
    "        return np.sin(3 * xs) * .45 + .55\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.max_position - self.min_position\n",
    "        scale = screen_width / world_width\n",
    "        carwidth = 40\n",
    "        carheight = 20\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            xs = np.linspace(self.min_position, self.max_position, 100)\n",
    "            ys = self._height(xs)\n",
    "            xys = list(zip((xs - self.min_position) * scale, ys * scale))\n",
    "\n",
    "            self.track = rendering.make_polyline(xys)\n",
    "            self.track.set_linewidth(4)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            clearance = 10\n",
    "\n",
    "            l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0\n",
    "            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            car.add_attr(rendering.Transform(translation=(0, clearance)))\n",
    "            self.cartrans = rendering.Transform()\n",
    "            car.add_attr(self.cartrans)\n",
    "            self.viewer.add_geom(car)\n",
    "            frontwheel = rendering.make_circle(carheight / 2.5)\n",
    "            frontwheel.set_color(.5, .5, .5)\n",
    "            frontwheel.add_attr(\n",
    "                rendering.Transform(translation=(carwidth / 4, clearance))\n",
    "            )\n",
    "            frontwheel.add_attr(self.cartrans)\n",
    "            self.viewer.add_geom(frontwheel)\n",
    "            backwheel = rendering.make_circle(carheight / 2.5)\n",
    "            backwheel.add_attr(\n",
    "                rendering.Transform(translation=(-carwidth / 4, clearance))\n",
    "            )\n",
    "            backwheel.add_attr(self.cartrans)\n",
    "            backwheel.set_color(.5, .5, .5)\n",
    "            self.viewer.add_geom(backwheel)\n",
    "            flagx = (self.goal_position-self.min_position) * scale\n",
    "            flagy1 = self._height(self.goal_position) * scale\n",
    "            flagy2 = flagy1 + 50\n",
    "            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))\n",
    "            self.viewer.add_geom(flagpole)\n",
    "            flag = rendering.FilledPolygon(\n",
    "                [(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)]\n",
    "            )\n",
    "            flag.set_color(.8, .8, 0)\n",
    "            self.viewer.add_geom(flag)\n",
    "\n",
    "        pos = self.state[0]\n",
    "        self.cartrans.set_translation(\n",
    "            (pos-self.min_position) * scale, self._height(pos) * scale\n",
    "        )\n",
    "        self.cartrans.set_rotation(math.cos(3 * pos))\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    # manual: python keyboard_agent.py MountainCar-v0, keys 0, 1, 2, ... to control (https://tomroth.com.au/gym-play/)\n",
    "    # Keyboard agent only supports discrete action spaces\n",
    "    def get_keys_to_action(self):\n",
    "        # Control with left and right arrow keys.\n",
    "        return {(): 1, (276,): 0, (275,): 2, (275, 276): 1}\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mymountaincar_dqn_agent\n",
    "\n",
    "import gym\n",
    "from stable_baselines import DQN\n",
    "\n",
    "#env = gym.make('MountainCar-v0')\n",
    "env = MyMountainCarEnv(0)\n",
    "\n",
    "training = True\n",
    "if training:\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"tensorboard_logs/mymountaincar_dqn_agent/\")\n",
    "    model.learn(total_timesteps=20000)  # normally at least 200000 steps\n",
    "    model.save(\"learned_models/dqn_mymountaincar\")\n",
    "else:\n",
    "    model = DQN.load(\"learned_models/dqn_mymountaincar\")\n",
    "    #model = DQN.load(\"learned_models/dqn_mymountaincar_reward_attempt3_200k_sb2\")\n",
    "\n",
    "print(\"finished training, now use the trained model and render the env\")\n",
    "\n",
    "n_episodes = 10\n",
    "for i in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_steps = 0\n",
    "    while not done:\n",
    "        action, state = model.predict(obs)  # greedy policy\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_steps += 1\n",
    "        env.render()\n",
    "\n",
    "    result = 'Success!' if episode_steps < 200 else 'Failure!'\n",
    "    print(result)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
